{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ee-hbExbtlTA"
   },
   "source": [
    "# CBA - RG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Eu_6MREItmXT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDefinition \\nFind rulesets that have support above minsup\\nRule-Item {condset, y}\\ncondsUpCount - Support Count of condset refers to the number of cases that contains condset \\nrulesUpCount - The support count of rule item refers to number of cases that contains condset with label y\\nSupport- rulesUpCount/datasize\\nConfidence - rulesUpCount/condsUpCount\\nFrequent dataset - If rule item satisfy > minSup (which is rulesUpCount/datasize)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Definition \n",
    "Find rulesets that have support above minsup\n",
    "Rule-Item {condset, y}\n",
    "condsUpCount - Support Count of condset refers to the number of cases that contains condset \n",
    "rulesUpCount - The support count of rule item refers to number of cases that contains condset with label y\n",
    "Support- rulesUpCount/datasize\n",
    "Confidence - rulesUpCount/condsUpCount\n",
    "Frequent dataset - If rule item satisfy > minSup (which is rulesUpCount/datasize)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "TcJiO8AstmKt",
    "outputId": "d4ae2a87-3e63-4b86-8526-1b647f584be8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFor all rule item that have same condset, the rule item with highest confidence (rulesUpCount/condsUpCount) is chosen\\nIf there is a tie, randomly select one rule-item (Maybe consider something else instead of random - Use the one with higher Support)\\nIf confidence of chosen rule is greater than minConf, we say the rule is accurate \\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "For all rule item that have same condset, the rule item with highest confidence (rulesUpCount/condsUpCount) is chosen\n",
    "If there is a tie, randomly select one rule-item (Maybe consider something else instead of random - Use the one with higher Support)\n",
    "If confidence of chosen rule is greater than minConf, we say the rule is accurate \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mtgc--CVwIz-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "agr0ExgOx4QO"
   },
   "source": [
    "# Importing the dataset & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DNsHJPgJyCpF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools    \n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQi1snaBpkwJ"
   },
   "source": [
    "# Preprocesing Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "aV62JmL089WA"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description: Recursive minimal entropy partitioning, to discretize continuous-valued attributes. We use the supervised\n",
    "    algorithm presented in Fayyad & Irani (1993) and introduced in Dougherty, Kohavi & Sahami (1995) section 3.3.\n",
    "    We also refer to a F# code on GitHub (https://gist.github.com/mathias-brandewinder/5650553).\n",
    "Input: a data table with several rows but only two column, the first column is continuous-valued (numerical) attributes,\n",
    "    and the second column is the class label of each data case (categorical).\n",
    "    e.g. data = [[1.0, 'Yes'], [0.5, 'No'], [2.0, 'Yes']]\n",
    "Output: a list of partition boundaries of the range of continuous-valued attribute in ascending sort order.\n",
    "    e.g. walls = [0.5, 0.8, 1.0], thus we can separate the range into 4 intervals: <=0.5, 0.5<*<=0.8, 0.8<*<=1.0 & >=1.0\n",
    "Author: CBA Studio\n",
    "Reference:\n",
    "    1. Multi-Interval Discretization of Continuous-Valued Attributes for Classification Learning, Fayyad & Irani, 1993\n",
    "    2. Supervised and Unsupervised Discretization of Continuous Features, Dougherty, Kohavi & Sahami, 1995\n",
    "    3. http://www.clear-lines.com/blog/post/Discretizing-a-continuous-variable-using-Entropy.aspx\n",
    "\"\"\"\n",
    "import math\n",
    "\n",
    "\n",
    "# A block to be split\n",
    "# It has 4 member:\n",
    "#   data: the data table with a column of continuous-valued attribute and a column of class label\n",
    "#   size: number of data case in this table\n",
    "#   number_of_classes: obviously, the number of class in this table\n",
    "#   entropy: entropy of dataset\n",
    "class Block:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.size = len(data)\n",
    "        classes = set([x[1] for x in data])     # get distinct class labels in this table\n",
    "        self.number_of_classes = len(set(classes))\n",
    "        self.entropy = calculate_entropy(data)\n",
    "\n",
    "\n",
    "# Calculate the entropy of dataset\n",
    "# parameter data: the data table to be used\n",
    "def calculate_entropy(data):\n",
    "    number_of_data = len(data)\n",
    "    classes = set([x[1] for x in data])\n",
    "    class_count = dict([(label, 0) for label in classes])\n",
    "    for data_case in data:\n",
    "        class_count[data_case[1]] += 1      # count the number of data case of each class\n",
    "    entropy = 0\n",
    "    for c in classes:\n",
    "        p = class_count[c] / number_of_data\n",
    "        entropy -= p * math.log2(p)         # calculate information entropy by its formula, where the base is 2\n",
    "    return entropy\n",
    "\n",
    "\n",
    "# Compute Gain(A, T: S) mentioned in Dougherty, Kohavi & Sahami (1995), i.e. entropy gained by splitting original_block\n",
    "#   into left_block and right_block\n",
    "# original_block: the block before partition\n",
    "# left_block: the block split which its value below boundary\n",
    "# right_block: the block above boundary\n",
    "def entropy_gain(original_block, left_block, right_block):\n",
    "    gain = original_block.entropy - \\\n",
    "            ((left_block.size / original_block.size) * left_block.entropy +\n",
    "            (right_block.size / original_block.size) * right_block.entropy)\n",
    "    return gain\n",
    "\n",
    "\n",
    "# Get minimum entropy gain required for a split of original_block into 2 blocks \"left\" and \"right\", see Dougherty,\n",
    "#   Kohavi & Sahami (1995)\n",
    "# original_block: the block before partition\n",
    "# left_block: the block split which its value below boundary\n",
    "# right_block: the block above boundary\n",
    "def min_gain(original_block, left_block, right_block):\n",
    "    delta = math.log2(math.pow(3, original_block.number_of_classes) - 2) - \\\n",
    "            (original_block.number_of_classes * original_block.entropy -\n",
    "             left_block.number_of_classes * left_block.entropy -\n",
    "             right_block.number_of_classes * right_block.entropy)\n",
    "    gain_sup = math.log2(original_block.size - 1) / original_block.size + delta / original_block.size\n",
    "    return gain_sup\n",
    "\n",
    "\n",
    "# Identify the best acceptable value to split block\n",
    "# block: a block of dataset\n",
    "# Return value: a list of (boundary, entropy gain, left block, right block) or\n",
    "#   None when it's unnecessary to split\n",
    "def split(block):\n",
    "    candidates = [x[0] for x in block.data]     # candidates is a list of values can be picked up as boundary\n",
    "    candidates = list(set(candidates))          # get different values in table\n",
    "    candidates.sort()                           # sort ascending\n",
    "    candidates = candidates[1:]                 # discard smallest, because by definition no value is smaller\n",
    "\n",
    "    wall = []       # wall is a list storing final boundary\n",
    "    for value in candidates:\n",
    "        # split by value into 2 groups, below & above\n",
    "        left_data = []\n",
    "        right_data = []\n",
    "        for data_case in block.data:\n",
    "            if data_case[0] < value:\n",
    "                left_data.append(data_case)\n",
    "            else:\n",
    "                right_data.append(data_case)\n",
    "\n",
    "        left_block = Block(left_data)\n",
    "        right_block = Block(right_data)\n",
    "\n",
    "        gain = entropy_gain(block, left_block, right_block)\n",
    "        threshold = min_gain(block, left_block, right_block)\n",
    "\n",
    "        # minimum threshold is met, the value is an acceptable candidate\n",
    "        if gain >= threshold:\n",
    "            wall.append([value, gain, left_block, right_block])\n",
    "\n",
    "    if wall:    # has candidate\n",
    "        wall.sort(key=lambda wall: wall[1], reverse=True)   # sort descending by \"gain\"\n",
    "        return wall[0]      # return best candidate with max entropy gain\n",
    "    else:\n",
    "        return None         # no need to split\n",
    "\n",
    "\n",
    "# Top-down recursive partition of a data block, append boundary into \"walls\"\n",
    "# block: a data block\n",
    "def partition(block):\n",
    "    walls = []\n",
    "\n",
    "    # inner recursive function, accumulate the partitioning values\n",
    "    # sub_block: just a data block\n",
    "    def recursive_split(sub_block):\n",
    "        wall_returned = split(sub_block)        # binary partition, get bin boundary\n",
    "        if wall_returned:                       # still can be spilt\n",
    "            walls.append(wall_returned[0])      # record this partitioning value\n",
    "            recursive_split(wall_returned[2])   # recursively process left block\n",
    "            recursive_split(wall_returned[3])   # recursively split right block\n",
    "        else:\n",
    "            return                              # end of recursion\n",
    "\n",
    "    recursive_split(block)      # call inner function\n",
    "    walls.sort()                # sort boundaries descending\n",
    "    return walls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-phdEn0m85Ar"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description: Pre-process original data. Firstly, we process the missing values (donated as '?'), discarding this column\n",
    "    when missing ratio above 50%, or filling blanks when below. We \"guess\" missing values by simply filling the mode of\n",
    "    existing values in the same column. And then, for the numerical attribute, we discretizate it by recursive minimal\n",
    "    entropy partitioning (see rmep.py). For the categorical attribute, we just replace the label with a\n",
    "    positive integer. For more information, see [1].\n",
    "Input: a data table with several data case, many attributes and class label in the last column, a list of the name of\n",
    "    each attribute, and a list of the type of each column.\n",
    "Output: a data list without numerical values and \"str\" categorical values.\n",
    "Author: CBA Studio\n",
    "Reference:\n",
    "    1. http://cgi.csc.liv.ac.uk/~frans/KDD/Software/LUCS-KDD-DN/lucs-kdd_DN.html\n",
    "\"\"\"\n",
    "# Identify the mode of a list, both effective for numerical and categorical list. When there exists too many modes\n",
    "#   having the same frequency, return the first one.\n",
    "# arr: a list need to find mode\n",
    "def get_mode(arr):\n",
    "    mode = []\n",
    "    arr_appear = dict((a, arr.count(a)) for a in arr)   # count appearance times of each key\n",
    "    if max(arr_appear.values()) == 1:       # if max time is 1\n",
    "        return      # no mode here\n",
    "    else:\n",
    "        for k, v in arr_appear.items():     # else, mode is the number which has max time\n",
    "            if v == max(arr_appear.values()):\n",
    "                mode.append(k)\n",
    "    return mode[0]  # return first number if has many modes\n",
    "\n",
    "\n",
    "# Fill missing values in column column_no, when missing values ration below 50%.\n",
    "# data: original data list\n",
    "# column_no: identify the column No. of that to be filled\n",
    "def fill_missing_values(data, column_no):\n",
    "    size = len(data)\n",
    "    column_data = [x[column_no] for x in data]      # get that column\n",
    "    while '?' in column_data:\n",
    "        column_data.remove('?')\n",
    "    mode = get_mode(column_data)\n",
    "    for i in range(size):\n",
    "        if data[i][column_no] == '?':\n",
    "            data[i][column_no] = mode              # fill in mode\n",
    "    return data\n",
    "\n",
    "\n",
    "# Get the list needed by rmep.py, just glue the data column with class column.\n",
    "# data_column: the data column\n",
    "# class_column: the class label column\n",
    "def get_discretization_data(data_column, class_column):\n",
    "    size = len(data_column)\n",
    "    result_list = []\n",
    "    for i in range(size):\n",
    "        result_list.append([data_column[i], class_column[i]])\n",
    "    return result_list\n",
    "\n",
    "\n",
    "# Replace numerical data with the No. of interval, i.e. consecutive positive integers.\n",
    "# data: original data table\n",
    "# column_no: the column No. of that column\n",
    "# walls: the split point of the whole range\n",
    "def replace_numerical(data, column_no, walls):\n",
    "    size = len(data)\n",
    "    num_spilt_point = len(walls)\n",
    "    for i in range(size):\n",
    "        if data[i][column_no] > walls[num_spilt_point - 1]:\n",
    "            data[i][column_no] = num_spilt_point + 1\n",
    "            continue\n",
    "        for j in range(0, num_spilt_point):\n",
    "            if data[i][column_no] <= walls[j]:\n",
    "                data[i][column_no] = j + 1\n",
    "                break\n",
    "    return data\n",
    "\n",
    "\n",
    "# Replace categorical values with a positive integer.\n",
    "# data: original data table\n",
    "# column_no: identify which column to be processed\n",
    "def replace_categorical(data, column_no):\n",
    "    size = len(data)\n",
    "    classes = set([x[column_no] for x in data])\n",
    "    classes_no = dict([(label, 0) for label in classes])\n",
    "    j = 1\n",
    "    for i in classes:\n",
    "        classes_no[i] = j\n",
    "        j += 1\n",
    "    for i in range(size):\n",
    "        data[i][column_no] = classes_no[data[i][column_no]]\n",
    "    return data, classes_no\n",
    "\n",
    "\n",
    "# Discard all the column with its column_no in discard_list\n",
    "# data: original data set\n",
    "# discard_list: a list of column No. of the columns to be discarded\n",
    "def discard(data, discard_list):\n",
    "    size = len(data)\n",
    "    length = len(data[0])\n",
    "    data_result = []\n",
    "    for i in range(size):\n",
    "        data_result.append([])\n",
    "        for j in range(length):\n",
    "            if j not in discard_list:\n",
    "                data_result[i].append(data[i][j])\n",
    "    return data_result\n",
    "\n",
    "\n",
    "# Main method here, see Description in detail\n",
    "# data: original data table\n",
    "# attribute: a list of the name of attribute\n",
    "# value_type: a list identifying the type of each column\n",
    "# Returned value: a data table after process\n",
    "def pre_process(data, attribute, value_type):\n",
    "    column_num = len(data[0])\n",
    "    size = len(data)\n",
    "    class_column = [x[-1] for x in data]\n",
    "    discard_list = []\n",
    "    for i in range(0, column_num - 1):\n",
    "        data_column = [x[i] for x in data]\n",
    "\n",
    "        # process missing values\n",
    "        missing_values_ratio = data_column.count('?') / size\n",
    "        if missing_values_ratio > 0.5:\n",
    "            discard_list.append(i)\n",
    "            continue\n",
    "        elif missing_values_ratio > 0:\n",
    "            data = fill_missing_values(data, i)\n",
    "            data_column = [x[i] for x in data]\n",
    "\n",
    "        # discretization\n",
    "        if value_type[i] == 'numerical':\n",
    "            discretization_data = get_discretization_data(data_column, class_column)\n",
    "            block = Block(discretization_data)\n",
    "            walls = partition(block)\n",
    "            if len(walls) == 0:\n",
    "                max_value = max(data_column)\n",
    "                min_value = min(data_column)\n",
    "                step = (max_value - min_value) / 3\n",
    "                walls.append(min_value + step)\n",
    "                walls.append(min_value + 2 * step)\n",
    "            print(attribute[i] + \":\", walls)        # print out split points\n",
    "            data = replace_numerical(data, i, walls)\n",
    "        elif value_type[i] == 'categorical':\n",
    "            data, classes_no = replace_categorical(data, i)\n",
    "            print(attribute[i] + \":\", classes_no)   # print out replacement list\n",
    "\n",
    "    # discard\n",
    "    if len(discard_list) > 0:\n",
    "        data = discard(data, discard_list)\n",
    "        print(\"discard:\", discard_list)             # print out discard list\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blEFJeTUXHgu"
   },
   "source": [
    "# Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Q5xHNoGix5Mh"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>13.71</td>\n",
       "      <td>5.65</td>\n",
       "      <td>2.45</td>\n",
       "      <td>20.5</td>\n",
       "      <td>95</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.06</td>\n",
       "      <td>7.70</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.74</td>\n",
       "      <td>740</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>13.40</td>\n",
       "      <td>3.91</td>\n",
       "      <td>2.48</td>\n",
       "      <td>23.0</td>\n",
       "      <td>102</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.41</td>\n",
       "      <td>7.30</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.56</td>\n",
       "      <td>750</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>13.27</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.35</td>\n",
       "      <td>10.20</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.56</td>\n",
       "      <td>835</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>13.17</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.37</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.46</td>\n",
       "      <td>9.30</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.62</td>\n",
       "      <td>840</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>14.13</td>\n",
       "      <td>4.10</td>\n",
       "      <td>2.74</td>\n",
       "      <td>24.5</td>\n",
       "      <td>96</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.35</td>\n",
       "      <td>9.20</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.60</td>\n",
       "      <td>560</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1     2     3    4     5     6     7     8      9     10    11  \\\n",
       "0    14.23  1.71  2.43  15.6  127  2.80  3.06  0.28  2.29   5.64  1.04  3.92   \n",
       "1    13.20  1.78  2.14  11.2  100  2.65  2.76  0.26  1.28   4.38  1.05  3.40   \n",
       "2    13.16  2.36  2.67  18.6  101  2.80  3.24  0.30  2.81   5.68  1.03  3.17   \n",
       "3    14.37  1.95  2.50  16.8  113  3.85  3.49  0.24  2.18   7.80  0.86  3.45   \n",
       "4    13.24  2.59  2.87  21.0  118  2.80  2.69  0.39  1.82   4.32  1.04  2.93   \n",
       "..     ...   ...   ...   ...  ...   ...   ...   ...   ...    ...   ...   ...   \n",
       "173  13.71  5.65  2.45  20.5   95  1.68  0.61  0.52  1.06   7.70  0.64  1.74   \n",
       "174  13.40  3.91  2.48  23.0  102  1.80  0.75  0.43  1.41   7.30  0.70  1.56   \n",
       "175  13.27  4.28  2.26  20.0  120  1.59  0.69  0.43  1.35  10.20  0.59  1.56   \n",
       "176  13.17  2.59  2.37  20.0  120  1.65  0.68  0.53  1.46   9.30  0.60  1.62   \n",
       "177  14.13  4.10  2.74  24.5   96  2.05  0.76  0.56  1.35   9.20  0.61  1.60   \n",
       "\n",
       "       12  13  \n",
       "0    1065   1  \n",
       "1    1050   1  \n",
       "2    1185   1  \n",
       "3    1480   1  \n",
       "4     735   1  \n",
       "..    ...  ..  \n",
       "173   740   3  \n",
       "174   750   3  \n",
       "175   835   3  \n",
       "176   840   3  \n",
       "177   560   3  \n",
       "\n",
       "[178 rows x 14 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german = pd.read_csv('german.data', header = None)\n",
    "iris = pd.read_csv('iris.data', header = None)\n",
    "wine = pd.read_csv('wine.data', header = None)\n",
    "zoo = pd.read_csv('zoo.data', header = None)\n",
    "bank = pd.read_csv('banknote_authentication.data', header = None)\n",
    "diabetes = pd.read_csv('pima-indians-diabetes.data', header = None)\n",
    "#user_knowledge = pd.read_csv('user-knowledge-modeling.data', header = None)\n",
    "wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "rrmvREopyHNM"
   },
   "outputs": [],
   "source": [
    "# transactions = german.values\n",
    "# transactions = iris.values\n",
    "transactions = wine.values\n",
    "# transactions = zoo.values\n",
    "# transactions = bank.values\n",
    "# transactions = diabetes.values\n",
    "# transactions = user_knowledge.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CSgr8xzx9QFS",
    "outputId": "827c16ae-36b0-4150-9dfb-7512a9e39e5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alcohol: [12.2, 12.79]\n",
      "Malic acid: [1.43, 2.31]\n",
      "Ash: [2.04]\n",
      "Alcalinity of ash: [18.0]\n",
      "Magnesium: [89.0]\n",
      "Total phenols: [1.85, 2.35]\n",
      "Flavanoids: [0.99, 1.58, 2.33]\n",
      "Nonflavanoid phenols: [0.4]\n",
      "Proanthocyanins: [1.28]\n",
      "Color intensity: [3.52, 7.6]\n",
      "Hue: [0.79, 0.98, 1.31]\n",
      "OD280/OD315 of diluted wines: [2.12, 2.48]\n",
      "Proline: [470.0, 760.0, 990.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3., 2., 2., ..., 3., 4., 1.],\n",
       "       [3., 2., 2., ..., 3., 4., 1.],\n",
       "       [3., 3., 2., ..., 3., 4., 1.],\n",
       "       ...,\n",
       "       [3., 3., 2., ..., 1., 3., 3.],\n",
       "       [3., 3., 2., ..., 1., 3., 3.],\n",
       "       [3., 3., 2., ..., 1., 2., 3.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# German Dataset\n",
    "# test_attribute = ['Status of existing checking account','Duration in month','Credit history','Purpose','Credit amount','Savings account/bonds','Present employment since','Installment rate in percentage of disposable income','Personal status and sex','Other debtors / guarantors','Present residence since','Property','Age in years','Other installment plans','Housing','Number of existing credits at this bank','Job','Number of people being liable to provide maintenance for','Telephone','foreign worker','class']\n",
    "# test_value_type = ['categorical','numerical','categorical','categorical','numerical','categorical','categorical','numerical','categorical','categorical','numerical','categorical','numerical','categorical','categorical','numerical','categorical','numerical','categorical','categorical','label']\n",
    "# test_data_after = pre_process(transactions, test_attribute, test_value_type)\n",
    "\n",
    "# Iris Dataset\n",
    "# test_attribute = ['sepal length','sepal width','petal length','petal width','class']\n",
    "# test_value_type = ['numerical','numerical','numerical','numerical','label']\n",
    "# test_data_after = pre_process(transactions, test_attribute, test_value_type)\n",
    "# test_data_after\n",
    "\n",
    "# Wine Dataset\n",
    "test_attribute = ['Alcohol','Malic acid','Ash','Alcalinity of ash','Magnesium','Total phenols','Flavanoids','Nonflavanoid phenols','Proanthocyanins' ,'Color intensity','Hue' ,'OD280/OD315 of diluted wines','Proline','class']\n",
    "test_value_type = ['numerical','numerical','numerical','numerical','numerical','numerical','numerical','numerical','numerical','numerical','numerical','numerical','numerical','label']\n",
    "test_data_after = pre_process(transactions, test_attribute, test_value_type)\n",
    "test_data_after\n",
    "\n",
    "# Zoo Dataset\n",
    "# test_attribute = ['hair','feathers','eggs','milk','airborne','aquatic','predator','toothed','backbone','breathes','enomous','fins','legs','tail','domestic','catsize','class']\n",
    "# test_value_type = ['categorical','categorical','categorical','categorical','categorical','categorical','categorical','categorical','categorical','categorical','categorical','categorical','numerical', 'categorical','categorical','categorical','label']\n",
    "# test_data_after = pre_process(transactions, test_attribute, test_value_type)\n",
    "# test_data_after\n",
    "\n",
    "# Bank Dataset\n",
    "# test_attribute = ['variance of Wavelet Transformed image','skewness of Wavelet Transformed image','curtosis of Wavelet Transformed image','entropy of image','class']\n",
    "# test_value_type = ['numerical','numerical','numerical','numerical','label']\n",
    "# test_data_after = pre_process(transactions, test_attribute, test_value_type)\n",
    "# test_data_after\n",
    "\n",
    "# Diabetes Dataset\n",
    "# test_attribute = ['Number of times pregnant','Plasma glucose concentration a 2 hours in an oral glucose tolerance test','Diastolic blood pressure','Triceps skin fold thickness','2-Hour serum insulin','Body mass index','Diabetes pedigree function','Age','class']\n",
    "# test_value_type = ['numerical','numerical','numerical','numerical','numerical','numerical','numerical','numerical','label']\n",
    "# test_data_after = pre_process(transactions, test_attribute, test_value_type)\n",
    "# test_data_after\n",
    "\n",
    "# User Knowledge Modelling Dataset\n",
    "# test_attribute = ['STG','SCG','STR','LPR','PEG','class']\n",
    "# test_value_type = ['numerical','numerical','numerical','numerical','numerical','label']\n",
    "# test_data_after = pre_process(transactions, test_attribute, test_value_type)\n",
    "# test_data_after\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 14)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(test_attribute), len(test_value_type))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_18wQWg-0HQ"
   },
   "source": [
    "# CBG - RG Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "57DNzfcj0Poj"
   },
   "outputs": [],
   "source": [
    "class CBA_RG():\n",
    "  # Generate the large frequent item step\n",
    "  def __init__(self, min_sup, transactions, confidence):\n",
    "    self.min_sup = min_sup\n",
    "    self.min_conf = confidence\n",
    "    # Pass in an array of transactions - Generate freq 1-itemset\n",
    "    freq_data_set = {}\n",
    "    self.transaction_length = len(transactions)\n",
    "    self.total_rules_seen = 0\n",
    "    # First Pass \n",
    "    item_sets, rules_up_count = self.first_pass(transactions)\n",
    "    car_1 , scores = self.gen_rules(item_sets,rules_up_count)\n",
    "    myResults = self.get_freq_items_list(item_sets, True)\n",
    "    self.first = myResults\n",
    "    self.all_car,self.get_all_scores = self.next_pass(myResults, transactions) # Subsequent Passes evaluated in next_pass\n",
    "    self.car_first = car_1\n",
    "    self.scores = scores\n",
    "    # <(condset, condsupCount), (y, rulesupCount)>\n",
    "    # Optional Rule Pruning could be done here\n",
    "  \n",
    "  # Rule Itemset -> {column_cat, value, label : count}\n",
    "  def first_pass(self, data):\n",
    "    item_sets = {}\n",
    "    rules_up_count = {}\n",
    "    for transaction in data:\n",
    "      label = transaction[-1]\n",
    "      for col in range(len(transaction)-1):\n",
    "        # <(condset, condsupCount), (y, rulesupCount)>\n",
    "        itemset = (col, transaction[col])\n",
    "        temp = item_sets.get(itemset, 0) + 1 # update the cond count\n",
    "        tempRuleUpCount = rules_up_count.get((itemset,label), 0) + 1 # update the rules up count\n",
    "        item_sets[itemset] = temp\n",
    "        rules_up_count[(itemset,label)] = tempRuleUpCount\n",
    "    for x in rules_up_count.copy():\n",
    "      if (rules_up_count[x]/self.transaction_length) < self.min_sup: # There could be 2 rules that has same cond \n",
    "        del rules_up_count[x]\n",
    "  \n",
    "    # We look for the cond support for item_sets now -> If rules_up_count doesn't have it, we delete freq\n",
    "    for x in item_sets.copy():\n",
    "      delete = True\n",
    "      for y in rules_up_count: # If there exists an existing rule, then we keep this frequent set \n",
    "        if y[0] == x:\n",
    "          delete = False\n",
    "      \n",
    "      if delete:\n",
    "        del item_sets[x]\n",
    "    # item_sets keep track of condset & condsUpCount\n",
    "    # rules_up_count keeps track of rules count\n",
    "    return item_sets, rules_up_count\n",
    "\n",
    "  def gen_freq_rules(self, freq_set, rules_set):\n",
    "    for x in rules_set.copy():\n",
    "      # x is the condset (column_1, value)\n",
    "      if rules_set[x]/self.transaction_length < self.min_sup: # Test for Infrequent and delete\n",
    "        if x in rules_set:\n",
    "          del rules_set[x]\n",
    "\n",
    "    for x in freq_set.copy():\n",
    "      delete = True\n",
    "      for y in rules_set: \n",
    "        if y[0] == x: # We can find a matching ruleset \n",
    "          delete = False\n",
    "      \n",
    "      if delete:\n",
    "        del freq_set[x]\n",
    "\n",
    "    return freq_set, rules_set\n",
    "\n",
    "  def gen_rules(self, freq_set, rules_set):\n",
    "    # We know it's frequent, now we need see if it's confident before generating rule\n",
    "    car = {}\n",
    "    car_confid_supp = {}\n",
    "    for x in rules_set:\n",
    "      if self.total_rules_seen % 25000 == 0 and self.total_rules_seen > 0 :\n",
    "        print(\"Total Rules Seen \" + str(self.total_rules_seen))\n",
    "      self.total_rules_seen += 1\n",
    "      if self.total_rules_seen > 80000: # If more than 80000 rules generated, we stop generating rules\n",
    "        break\n",
    "      # x is the condset (column_1, value)\n",
    "      if rules_set[x]/self.transaction_length >= self.min_sup: # Test for frequent again\n",
    "        if x[0] not in car: # If A -> B not in car, then we assigned it into car and we check for confidence \n",
    "          confidence_temp = rules_set[x]/freq_set[x[0]]\n",
    "          if confidence_temp > self.min_conf: # We want the rule to be accurate as well\n",
    "            car[x[0]] = x[1] # LHS => RHS\n",
    "            car_confid_supp[x] = [rules_set[x]/freq_set[x[0]] , rules_set[x]/self.transaction_length] \n",
    "        else: \n",
    "          cur_car_right = car[x[0]] # Get the current best rule \n",
    "          temp = (x[0], cur_car_right)\n",
    "          cur_car_confidence = rules_set[temp] / freq_set[x[0]]\n",
    "          cond_score = rules_set[x]/freq_set[x[0]]\n",
    "          if cond_score > cur_car_confidence: \n",
    "            # We change rule - And we know confidence it's greater than min_confd since original set has to be more than and this new set has more than orignial set\n",
    "              car[x[0]] = x[1]\n",
    "              car_confid_supp[x] = [rules_set[x]/freq_set[x[0]], rules_set[x]/self.transaction_length]\n",
    "          pass\n",
    "    return car, car_confid_supp\n",
    " \n",
    "  # Counts items and class occurences to determine freq 1-itemset\n",
    "  def get_freq_items_list(self, full_set, first=False):\n",
    "      temp_arr = []\n",
    "      for x in full_set:\n",
    "        temp_arr.append([x])\n",
    "      return temp_arr\n",
    "\n",
    "  def next_pass(self , freq_item, transactions):\n",
    "    k = 1\n",
    "    car_rules = []\n",
    "    car_confid_supp_scores = []\n",
    "    transaction_info = self.rule_subset_prepare(transactions)\n",
    "    while freq_item: \n",
    "      candidate_sets = self.candidate_gen(freq_item, freq_item, k) \n",
    "      # Process candidate to tuple \n",
    "      candidate_sets_tuple = self.gen_cand_tuple(candidate_sets)\n",
    "      rules_up_count = {}\n",
    "      hash_candidates = self.generate_hash_candidates(candidate_sets)\n",
    "      count = 0\n",
    "      for transaction in transactions:\n",
    "        # We generate all the subset found \n",
    "        results_candidate = self.rule_subset1(candidate_sets_tuple, transaction, transaction_info[count], k)\n",
    "        # We do an increase in count in both support & rule\n",
    "        for candidate in results_candidate: \n",
    "          temp = hash_candidates[candidate] + 1\n",
    "          hash_candidates[candidate] = temp # Increase in support count\n",
    "          temp1 = rules_up_count.get((candidate, results_candidate[candidate]), 0) + 1\n",
    "          rules_up_count[(candidate, results_candidate[candidate])] = temp1\n",
    "        count += 1\n",
    "      hash_candidates = self.postprocess_hash(hash_candidates)\n",
    "      freq_set, rules_up_count = self.gen_freq_rules(hash_candidates, rules_up_count)\n",
    "      rules, scores = self.gen_rules(freq_set, rules_up_count)\n",
    "      car_rules.append(rules)\n",
    "      car_confid_supp_scores.append(scores)\n",
    "      k += 1\n",
    "      freq_item = [v for v in freq_set.keys()]# Assign dictionary as array\n",
    "      # Candidate Pruning\n",
    "      if self.total_rules_seen > 80000:\n",
    "        print(\"Maximum Rules Reached\")\n",
    "        break\n",
    "    return car_rules, car_confid_supp_scores\n",
    "\n",
    "  def postprocess_hash(self, hash_cand):\n",
    "    for x in hash_cand.copy():\n",
    "      if hash_cand[x] == 0: # Means no support at all\n",
    "        del hash_cand[x]\n",
    "    return hash_cand\n",
    "\n",
    "  def generate_hash_candidates(self, candidate_sets):\n",
    "    myDict = {}\n",
    "    for x in candidate_sets:\n",
    "      myDict[tuple(x)] = 0\n",
    "    return myDict\n",
    "\n",
    "  def rule_subset_prepare(self, transactions):\n",
    "    transaction_info = []\n",
    "    for transaction in transactions: \n",
    "      temp = []\n",
    "      for x in range(len(transaction)-1):\n",
    "        temp.append((x, transaction[x]))\n",
    "      transaction_info.append(temp)\n",
    "    return transaction_info\n",
    "\n",
    "# We find if candidate rules exists in transactions or not \n",
    "  def rule_subset1(self, candidate_sets, transaction, transaction_row, k):\n",
    "    result = {}\n",
    "    for x in candidate_sets:\n",
    "      satisfy = True\n",
    "      for single_rule in x:\n",
    "        if single_rule[1] != transaction[single_rule[0]]: # not a support\n",
    "          satisfy = False\n",
    "      if satisfy: \n",
    "        result[x] = transaction[-1]\n",
    "    return result\n",
    "\n",
    "# Helper function to convert candidate set\n",
    "  def gen_cand_tuple(self, candidate_sets):\n",
    "    temp = [] \n",
    "    for x in candidate_sets:\n",
    "      temp.append(tuple(x))\n",
    "    return temp\n",
    "\n",
    "# Generate the candidate sets\n",
    "  def candidate_gen(self, freq_item, freq_item_2, k):\n",
    "    candidate_set = []     # condition of joining is that is should have k-1 elements in common \n",
    "    for x in range(len(freq_item)):\n",
    "      temp = freq_item[x]\n",
    "      for y in range(x+1, len(freq_item_2)):\n",
    "        if x ==y: #Ignore the same key\n",
    "          continue \n",
    "        temp2 = freq_item_2[y]\n",
    "        # Check if the rules are the same\n",
    "        if k == 1:\n",
    "          temp_merged = temp[:]\n",
    "          temp_merged.append(temp2[-1])\n",
    "          temp_merged = tuple(temp_merged)\n",
    "          if temp_merged not in candidate_set:\n",
    "            candidate_set.append(temp_merged)\n",
    "        elif temp[:-1] == temp2[:-1]:\n",
    "          temp_merged = temp + (temp2[-1],)    # Merge the 2\n",
    "        # If exists in candidate set then we append the set\n",
    "          if temp_merged not in candidate_set:\n",
    "            candidate_set.append(temp_merged)\n",
    "    return candidate_set\n",
    "\n",
    "  def post_process(self): # prepare data for classification \n",
    "    car_first = self.car_first\n",
    "    car_all = self.all_car[:]\n",
    "    scores_all = self.get_all_scores[:]\n",
    "    scores_all.insert(0, self.scores)\n",
    "    count = 0\n",
    "    car_all.insert(0, car_first)\n",
    "    myDict = {}\n",
    "    for x in car_all:\n",
    "      for item in x:\n",
    "        myDict[(item, x[item])] = scores_all[count][(item , x[item])]\n",
    "      count+=1 \n",
    "    newDict = {} # Intermediate dict to format to pandas for easier sorting \n",
    "    count = 0\n",
    "    for x in myDict:\n",
    "      temp = myDict[x] \n",
    "      temp.insert(0,x)\n",
    "      newDict[count] = temp\n",
    "      count += 1\n",
    "\n",
    "    # Sort my rules \n",
    "    new = pd.DataFrame.from_dict(newDict,  orient ='index', columns = ['rule', 'confidence', 'support'])\n",
    "    df = new.sort_values(['confidence','support'], ascending=[False, False])\n",
    "    return df.values \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yC60vx_8sJHg"
   },
   "source": [
    "# CBA-CB: M1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "CKhSRBx1vqsh"
   },
   "outputs": [],
   "source": [
    "class CBA_CB_M1:\n",
    "  def __init__(self, my_rules, transactions):\n",
    "    self.my_rules = my_rules\n",
    "    self.transactions = transactions\n",
    "\n",
    "  def generate_classifier(self):\n",
    "    prev_error = []\n",
    "    total_error = []\n",
    "    ruleList = []\n",
    "    default_classes = []\n",
    "    transactions_used = self.transactions[:]\n",
    "    for rule in self.my_rules:\n",
    "      temp = []\n",
    "      actual_rule = rule[0] # Leave out the scores \n",
    "      if type(actual_rule[0][0]) is tuple: # contains only one tuple \n",
    "        single = False \n",
    "      else:\n",
    "        single = True\n",
    "      pred = actual_rule[-1] # gets the label of rule \n",
    "      actual_rule = actual_rule[0] # gets the LHS of rule \n",
    "      count = error = 0\n",
    "      pred_right_once = False\n",
    "      if len(transactions_used) == 0:\n",
    "        break\n",
    "\n",
    "      for transaction in transactions_used:\n",
    "        if single:\n",
    "          if transaction[actual_rule[0]] == actual_rule[1]:\n",
    "            if transaction[-1] == pred: # classify at least one correctly \n",
    "              pred_right_once = True\n",
    "            else:\n",
    "              error += 1\n",
    "            temp.append(count) # save the marked temp to decide whether to delete at the end or not \n",
    "        else:\n",
    "          for single_col_rule in actual_rule:\n",
    "            satisfy = True\n",
    "            if transaction[single_col_rule[0]] != single_col_rule[1]:\n",
    "              satisfy = False\n",
    "          if satisfy: # Add rule \n",
    "            if transaction[-1] == pred: \n",
    "              pred_right_once = True\n",
    "            else:\n",
    "              error += 1\n",
    "            temp.append(count) # Prepare to delete from list \n",
    "        count += 1\n",
    "\n",
    "      if pred_right_once:\n",
    "      # delete index in transactions\n",
    "        new_transaction = []\n",
    "        temp_counter = 0\n",
    "        for item in transactions_used:\n",
    "          if temp_counter not in temp:\n",
    "              new_transaction.append(item)\n",
    "          temp_counter += 1\n",
    "        transactions_used = new_transaction[:]\n",
    "\n",
    "        # add r to rule_list \n",
    "        ruleList.append((actual_rule, pred))\n",
    "\n",
    "        my_dict = {}\n",
    "        # add majority in remaining as default class\n",
    "        for remaining in new_transaction:\n",
    "          temp = my_dict.get(remaining[-1],0) + 1\n",
    "          my_dict[remaining[-1]] = temp\n",
    "\n",
    "        # find majority class \n",
    "        maj_class = [key for key,value in my_dict.items() if value == max(my_dict.values())]\n",
    "        if len(maj_class) > 0:\n",
    "          maj_class = maj_class[0]\n",
    "        default_classes.append(maj_class)\n",
    "\n",
    "        # calculate total number of error for default class\n",
    "        default_errors = 0\n",
    "        for remaining in new_transaction:\n",
    "          if remaining[-1] == maj_class:\n",
    "            continue\n",
    "          else:\n",
    "            default_errors += 1\n",
    "        # print(\"My error \" + str(error))\n",
    "        # print(\"Default error \" + str(default_errors))\n",
    "        # error is the classes that this rule classify wrongly\n",
    "        \n",
    "        if len(prev_error) == 0: # First Rule Error  \n",
    "            total_error.append(default_errors + error)\n",
    "            prev_error.append(error)\n",
    "        else: # Subsequent rules error \n",
    "            total_error.append(default_errors + error + prev_error[-1])\n",
    "            prev_error.append(sum(prev_error) + error)\n",
    "\n",
    "    return ruleList, prev_error, total_error , default_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "PMr0MVcH8iB5"
   },
   "outputs": [],
   "source": [
    "# Evaluate Test Set\n",
    "def evaluate_cba_m1(rules, test_set, default_class):\n",
    "  error = 0\n",
    "  for rule in rules:\n",
    "    actual_rule = rule[0] # Leave out the scores \n",
    "    if type(actual_rule[0]) is tuple: # contains only one tuple \n",
    "      single = False \n",
    "    else:\n",
    "      actual_rule = tuple((actual_rule,))\n",
    "      single = True\n",
    "    # Check if it fulfill test set\n",
    "    pred = rule[-1]\n",
    "    index_del = [] # First find if rule exists for a transaction \n",
    "    count = 0\n",
    "    for test in test_set:\n",
    "      satisfy = True\n",
    "      for single_rule in actual_rule:\n",
    "        if single_rule[1] != test[single_rule[0]]:\n",
    "          satisfy = False\n",
    "      if satisfy:  # Means rule applies to this row\n",
    "        # We wanna delete this test index\n",
    "        # We wnna find error too\n",
    "        if pred != test[-1]: \n",
    "          error += 1\n",
    "        index_del.append(count)\n",
    "      count += 1\n",
    "    # Delete test from test_set\n",
    "    new_test = []\n",
    "    for z in range(len(test_set)):\n",
    "      if z not in index_del:\n",
    "        new_test.append(test_set[z])\n",
    "    test_set = new_test[:]\n",
    "  \n",
    "  for test in test_set: \n",
    "    if test[-1] != default_class:\n",
    "      error += 1\n",
    "\n",
    "  return error\n",
    "\n",
    "def evaluate_cba_m2(rules, test_set, default_class):\n",
    "  my_pred = []\n",
    "  true_label = []\n",
    "  for test in test_set:\n",
    "    true_label.append(test[-1])\n",
    "    for rule in rules:\n",
    "      actual_rule = rule[0] # Leave out the scores \n",
    "      if type(actual_rule[0]) is tuple: # contains only one tuple \n",
    "        single = False \n",
    "      else:\n",
    "        actual_rule = tuple((actual_rule,))\n",
    "        single = True\n",
    "\n",
    "      pred = rule[-1]\n",
    "      satisfy = True \n",
    "      for single_rule in actual_rule:\n",
    "        if single_rule[1] != test[single_rule[0]]:\n",
    "          satisfy = False\n",
    "      if satisfy:\n",
    "        my_pred.append(pred)# We found the rule and we want to break \n",
    "        break\n",
    "      \n",
    "    if not satisfy: # Means a rule was not found for this test, we resort to default \n",
    "      my_pred.append(default_class) # We predict default_class\n",
    "\n",
    "  return my_pred, true_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4QUyLNxysgA"
   },
   "source": [
    "# 10-Fold Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r1zSh5PFpo3c",
    "outputId": "33882048-0a78-4a84-c2cc-9717e51cb123"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Running for fold 0\n",
      "Total Rules Seen 25000\n",
      "Total Rules Seen 50000\n",
      "Total Rules Seen 75000\n",
      "Maximum Rules Reached\n",
      "\n",
      "Number of Rules Generated = 73169\n",
      "Number of Classifier Rules Generated 3\n",
      "Error from Building Classifier =51\n",
      "Error from Test  = 1\n",
      "\n",
      "Metrics - \n",
      "Accuracy Score :  0.9444444444444444\n",
      "Precision Score :  [0.83333333 1.         1.        ]\n",
      "Recall Score :  [1.    0.875 1.   ]\n",
      "F1 Score :  [0.90909091 0.93333333 1.        ]\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Running for fold 1\n",
      "Total Rules Seen 25000\n",
      "Total Rules Seen 50000\n",
      "Total Rules Seen 75000\n",
      "Maximum Rules Reached\n",
      "\n",
      "Number of Rules Generated = 73856\n",
      "Number of Classifier Rules Generated 2\n",
      "Error from Building Classifier =51\n",
      "Error from Test  = 5\n",
      "\n",
      "Metrics - \n",
      "Accuracy Score :  0.7222222222222222\n",
      "Precision Score :  [1.         0.71428571 0.4       ]\n",
      "Recall Score :  [0.75  0.625 1.   ]\n",
      "F1 Score :  [0.85714286 0.66666667 0.57142857]\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Running for fold 2\n",
      "Total Rules Seen 25000\n",
      "Total Rules Seen 50000\n",
      "Total Rules Seen 75000\n",
      "Maximum Rules Reached\n",
      "\n",
      "Number of Rules Generated = 73648\n",
      "Number of Classifier Rules Generated 2\n",
      "Error from Building Classifier =53\n",
      "Error from Test  = 6\n",
      "\n",
      "Metrics - \n",
      "Accuracy Score :  0.6666666666666666\n",
      "Precision Score :  [0.83333333 0.85714286 0.2       ]\n",
      "Recall Score :  [0.83333333 0.54545455 1.        ]\n",
      "F1 Score :  [0.83333333 0.66666667 0.33333333]\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Running for fold 3\n",
      "Total Rules Seen 25000\n",
      "Total Rules Seen 50000\n",
      "Total Rules Seen 75000\n",
      "Maximum Rules Reached\n",
      "\n",
      "Number of Rules Generated = 73181\n",
      "Number of Classifier Rules Generated 3\n",
      "Error from Building Classifier =53\n",
      "Error from Test  = 2\n",
      "\n",
      "Metrics - \n",
      "Accuracy Score :  0.8888888888888888\n",
      "Precision Score :  [0.83333333 0.85714286 1.        ]\n",
      "Recall Score :  [1.         0.85714286 0.83333333]\n",
      "F1 Score :  [0.90909091 0.85714286 0.90909091]\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Running for fold 4\n",
      "Total Rules Seen 25000\n",
      "Total Rules Seen 50000\n",
      "Total Rules Seen 75000\n",
      "Maximum Rules Reached\n",
      "\n",
      "Number of Rules Generated = 73844\n",
      "Number of Classifier Rules Generated 3\n",
      "Error from Building Classifier =38\n",
      "Error from Test  = 2\n",
      "\n",
      "Metrics - \n",
      "Accuracy Score :  0.8888888888888888\n",
      "Precision Score :  [0.83333333 0.85714286 1.        ]\n",
      "Recall Score :  [1.         0.85714286 0.83333333]\n",
      "F1 Score :  [0.90909091 0.85714286 0.90909091]\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Running for fold 5\n",
      "Total Rules Seen 25000\n",
      "Total Rules Seen 50000\n",
      "Total Rules Seen 75000\n",
      "Maximum Rules Reached\n",
      "\n",
      "Number of Rules Generated = 73802\n",
      "Number of Classifier Rules Generated 3\n",
      "Error from Building Classifier =46\n",
      "Error from Test  = 1\n",
      "\n",
      "Metrics - \n",
      "Accuracy Score :  0.9444444444444444\n",
      "Precision Score :  [1.         0.85714286 1.        ]\n",
      "Recall Score :  [1.         1.         0.83333333]\n",
      "F1 Score :  [1.         0.92307692 0.90909091]\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Running for fold 6\n",
      "Total Rules Seen 25000\n",
      "Total Rules Seen 50000\n",
      "Total Rules Seen 75000\n",
      "Maximum Rules Reached\n",
      "\n",
      "Number of Rules Generated = 72935\n",
      "Number of Classifier Rules Generated 3\n",
      "Error from Building Classifier =48\n",
      "Error from Test  = 0\n",
      "\n",
      "Metrics - \n",
      "Accuracy Score :  1.0\n",
      "Precision Score :  [1. 1. 1.]\n",
      "Recall Score :  [1. 1. 1.]\n",
      "F1 Score :  [1. 1. 1.]\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Running for fold 7\n",
      "Total Rules Seen 25000\n",
      "Total Rules Seen 50000\n",
      "Total Rules Seen 75000\n",
      "Maximum Rules Reached\n",
      "\n",
      "Number of Rules Generated = 73138\n",
      "Number of Classifier Rules Generated 3\n",
      "Error from Building Classifier =47\n",
      "Error from Test  = 0\n",
      "\n",
      "Metrics - \n",
      "Accuracy Score :  1.0\n",
      "Precision Score :  [1. 1. 1.]\n",
      "Recall Score :  [1. 1. 1.]\n",
      "F1 Score :  [1. 1. 1.]\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Running for fold 8\n",
      "Total Rules Seen 25000\n",
      "Total Rules Seen 50000\n",
      "Total Rules Seen 75000\n",
      "Maximum Rules Reached\n",
      "\n",
      "Number of Rules Generated = 73192\n",
      "Number of Classifier Rules Generated 3\n",
      "Error from Building Classifier =46\n",
      "Error from Test  = 2\n",
      "\n",
      "Metrics - \n",
      "Accuracy Score :  0.8823529411764706\n",
      "Precision Score :  [0.83333333 0.85714286 1.        ]\n",
      "Recall Score :  [1.         0.85714286 0.8       ]\n",
      "F1 Score :  [0.90909091 0.85714286 0.88888889]\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Running for fold 9\n",
      "Total Rules Seen 25000\n",
      "Total Rules Seen 50000\n",
      "Total Rules Seen 75000\n",
      "Maximum Rules Reached\n",
      "\n",
      "Number of Rules Generated = 73319\n",
      "Number of Classifier Rules Generated 3\n",
      "Error from Building Classifier =46\n",
      "Error from Test  = 0\n",
      "\n",
      "Metrics - \n",
      "Accuracy Score :  1.0\n",
      "Precision Score :  [1. 1. 1.]\n",
      "Recall Score :  [1. 1. 1.]\n",
      "F1 Score :  [1. 1. 1.]\n",
      "--------------------------------------------------\n",
      "Mean Error of 10CV is 1.9\n",
      "Mean CAR Rules generated from 10CV is 73408.4\n",
      "Number of Rules in Classifier from 10CV is 2.8\n",
      "Error from building classifier from 10CV is 1.9\n",
      "\n",
      "Mean Accuracy Score is 0.8937908496732027\n",
      "Mean Precision Score is [0.91666667 0.9        0.86      ]\n",
      "Mean Recall Score is [0.95833333 0.86168831 0.93      ]\n",
      "Mean F1 Score is [0.93268398 0.87611722 0.85209235]\n"
     ]
    }
   ],
   "source": [
    "# Split Data into 10 sets\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "X = [item[:-1] for item in test_data_after]\n",
    "y = [item[-1] for item in test_data_after]\n",
    "\n",
    "mean_error_train = 0\n",
    "mean_rules = 0\n",
    "mean_rules_classifier = 0\n",
    "mean_classifier_error = 0\n",
    "\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1s = []\n",
    "\n",
    "kf = StratifiedKFold(n_splits=10)\n",
    "fold_no = 0\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    print(\"--------------------------------------------------\")\n",
    "    print(\"Running for fold \" + str(fold_no))\n",
    "    my_train_data = []\n",
    "    for i in train_index: \n",
    "        temp_list = np.append(X[i], y[i]) \n",
    "        my_train_data.append(temp_list.tolist())\n",
    "\n",
    "    test_list = []\n",
    "    for i in test_index:\n",
    "        temp_list = np.append(X[i], y[i])\n",
    "        test_list.append(temp_list.tolist())\n",
    "\n",
    "    cba_rules = CBA_RG(0.01, my_train_data[:], 0.5) # Rule Generation happens in initialization\n",
    "\n",
    "    # my_rules - Ruleset, Confidence, Min_Sup\n",
    "    my_rules = cba_rules.post_process() #TODO After runnnig once, need to re_generate cba_rules\n",
    "\n",
    "    cba_classifier = CBA_CB_M1(my_rules, my_train_data)\n",
    "    ruleList, prev_error , total_error , default_classes= cba_classifier.generate_classifier()\n",
    "    min_errors_idx = total_error.index(min(total_error))\n",
    "    # We take rules up to that min_error rule -> and discard all rules after\n",
    "    mean_classifier_error += min(total_error)\n",
    "    ruleList = ruleList[:min_errors_idx+1]\n",
    "    error = evaluate_cba_m1(ruleList, test_list, default_classes[min_errors_idx])\n",
    "    predicted, actual = evaluate_cba_m2(ruleList, test_list, default_classes[min_errors_idx])\n",
    "    mean_rules += len(my_rules)\n",
    "    mean_rules_classifier += len(ruleList)\n",
    "    mean_error_train += error\n",
    "    fold_no += 1\n",
    "    print(\"\")\n",
    "    print(\"Number of Rules Generated = \" + str(len(my_rules)))\n",
    "    print(\"Number of Classifier Rules Generated \" + str(len(ruleList)))\n",
    "    print(\"Error from Building Classifier =\" + str(min(total_error)))\n",
    "    print(\"Error from Test  = \" + str(error))\n",
    "    print(\"\")\n",
    "    print(\"Metrics - \")\n",
    "    precisions.append(precision_score(predicted, actual, average = None))\n",
    "    recalls.append(recall_score(predicted, actual, average = None))\n",
    "    f1s.append(f1_score(predicted, actual, average = None))\n",
    "    accuracies.append(accuracy_score(predicted, actual))\n",
    "    print(\"Accuracy Score : \", (accuracy_score(predicted, actual)))\n",
    "    print(\"Precision Score : \", precision_score(predicted, actual, average = None, zero_division=0))\n",
    "    print(\"Recall Score : \", recall_score(predicted, actual, average = None, zero_division=0))\n",
    "    print(\"F1 Score : \", f1_score(predicted, actual, average = None, zero_division=0))\n",
    "    print(\"--------------------------------------------------\")\n",
    "    \n",
    "\n",
    "\n",
    "print(\"Mean Error of 10CV is \" + str(mean_error_train/10))\n",
    "print(\"Mean CAR Rules generated from 10CV is \" + str(mean_rules/10))\n",
    "print(\"Number of Rules in Classifier from 10CV is \" + str(mean_rules_classifier/10))\n",
    "print(\"Error from building classifier from 10CV is \" + str(mean_error_train/10))\n",
    "print(\"\")\n",
    "print(\"Mean Accuracy Score is\", np.sum(accuracies, axis=0)/len(accuracies))\n",
    "print(\"Mean Precision Score is\", np.sum(precisions, axis=0)/len(precisions))\n",
    "print(\"Mean Recall Score is\", np.sum(recalls, axis=0)/len(recalls))\n",
    "print(\"Mean F1 Score is\", np.sum(f1s, axis=0)/len(f1s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Ee-hbExbtlTA",
    "IQi1snaBpkwJ"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
