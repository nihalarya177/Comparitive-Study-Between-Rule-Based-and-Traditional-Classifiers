{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CBA - RG "
      ],
      "metadata": {
        "id": "Ee-hbExbtlTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Definition \n",
        "Find rulesets that have support above minsup\n",
        "Rule-Item {condset, y}\n",
        "condsUpCount - Support Count of condset refers to the number of cases that contains condset \n",
        "rulesUpCount - The support count of rule item refers to number of cases that contains condset with label y\n",
        "Support- rulesUpCount/datasize\n",
        "Confidence - rulesUpCount/condsUpCount\n",
        "Frequent dataset - If rule item satisfy > minSup (which is rulesUpCount/datasize)\n",
        "'''"
      ],
      "metadata": {
        "id": "Eu_6MREItmXT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "736f6b7e-516a-4e52-b308-b37c8bc6b66a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nDefinition \\nFind rulesets that have support above minsup\\nRule-Item {condset, y}\\ncondsUpCount - Support Count of condset refers to the number of cases that contains condset \\nrulesUpCount - The support count of rule item refers to number of cases that contains condset with label y\\nSupport- rulesUpCount/datasize\\nConfidence - rulesUpCount/condsUpCount\\nFrequent dataset - If rule item satisfy > minSup (which is rulesUpCount/datasize)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "For all rule item that have same condset, the rule item with highest confidence (rulesUpCount/condsUpCount) is chosen\n",
        "If there is a tie, randomly select one rule-item (Maybe consider something else instead of random - Use the one with higher Support)\n",
        "If confidence of chosen rule is greater than minConf, we say the rule is accurate \n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "TcJiO8AstmKt",
        "outputId": "8d8b6333-bb7b-4f51-b3ef-64fd08a330f0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nFor all rule item that have same condset, the rule item with highest confidence (rulesUpCount/condsUpCount) is chosen\\nIf there is a tie, randomly select one rule-item (Maybe consider something else instead of random - Use the one with higher Support)\\nIf confidence of chosen rule is greater than minConf, we say the rule is accurate \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mtgc--CVwIz-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing the dataset & Preprocessing"
      ],
      "metadata": {
        "id": "agr0ExgOx4QO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools    \n"
      ],
      "metadata": {
        "id": "DNsHJPgJyCpF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcvwgEP0yPb9",
        "outputId": "b8cbbdb7-eb60-4df7-8f9d-000b7f91e16f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocesing Helper Functions"
      ],
      "metadata": {
        "id": "IQi1snaBpkwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Description: Recursive minimal entropy partitioning, to discretize continuous-valued attributes. We use the supervised\n",
        "    algorithm presented in Fayyad & Irani (1993) and introduced in Dougherty, Kohavi & Sahami (1995) section 3.3.\n",
        "    We also refer to a F# code on GitHub (https://gist.github.com/mathias-brandewinder/5650553).\n",
        "Input: a data table with several rows but only two column, the first column is continuous-valued (numerical) attributes,\n",
        "    and the second column is the class label of each data case (categorical).\n",
        "    e.g. data = [[1.0, 'Yes'], [0.5, 'No'], [2.0, 'Yes']]\n",
        "Output: a list of partition boundaries of the range of continuous-valued attribute in ascending sort order.\n",
        "    e.g. walls = [0.5, 0.8, 1.0], thus we can separate the range into 4 intervals: <=0.5, 0.5<*<=0.8, 0.8<*<=1.0 & >=1.0\n",
        "Author: CBA Studio\n",
        "Reference:\n",
        "    1. Multi-Interval Discretization of Continuous-Valued Attributes for Classification Learning, Fayyad & Irani, 1993\n",
        "    2. Supervised and Unsupervised Discretization of Continuous Features, Dougherty, Kohavi & Sahami, 1995\n",
        "    3. http://www.clear-lines.com/blog/post/Discretizing-a-continuous-variable-using-Entropy.aspx\n",
        "\"\"\"\n",
        "import math\n",
        "\n",
        "\n",
        "# A block to be split\n",
        "# It has 4 member:\n",
        "#   data: the data table with a column of continuous-valued attribute and a column of class label\n",
        "#   size: number of data case in this table\n",
        "#   number_of_classes: obviously, the number of class in this table\n",
        "#   entropy: entropy of dataset\n",
        "class Block:\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        self.size = len(data)\n",
        "        classes = set([x[1] for x in data])     # get distinct class labels in this table\n",
        "        self.number_of_classes = len(set(classes))\n",
        "        self.entropy = calculate_entropy(data)\n",
        "\n",
        "\n",
        "# Calculate the entropy of dataset\n",
        "# parameter data: the data table to be used\n",
        "def calculate_entropy(data):\n",
        "    number_of_data = len(data)\n",
        "    classes = set([x[1] for x in data])\n",
        "    class_count = dict([(label, 0) for label in classes])\n",
        "    for data_case in data:\n",
        "        class_count[data_case[1]] += 1      # count the number of data case of each class\n",
        "    entropy = 0\n",
        "    for c in classes:\n",
        "        p = class_count[c] / number_of_data\n",
        "        entropy -= p * math.log2(p)         # calculate information entropy by its formula, where the base is 2\n",
        "    return entropy\n",
        "\n",
        "\n",
        "# Compute Gain(A, T: S) mentioned in Dougherty, Kohavi & Sahami (1995), i.e. entropy gained by splitting original_block\n",
        "#   into left_block and right_block\n",
        "# original_block: the block before partition\n",
        "# left_block: the block split which its value below boundary\n",
        "# right_block: the block above boundary\n",
        "def entropy_gain(original_block, left_block, right_block):\n",
        "    gain = original_block.entropy - \\\n",
        "            ((left_block.size / original_block.size) * left_block.entropy +\n",
        "            (right_block.size / original_block.size) * right_block.entropy)\n",
        "    return gain\n",
        "\n",
        "\n",
        "# Get minimum entropy gain required for a split of original_block into 2 blocks \"left\" and \"right\", see Dougherty,\n",
        "#   Kohavi & Sahami (1995)\n",
        "# original_block: the block before partition\n",
        "# left_block: the block split which its value below boundary\n",
        "# right_block: the block above boundary\n",
        "def min_gain(original_block, left_block, right_block):\n",
        "    delta = math.log2(math.pow(3, original_block.number_of_classes) - 2) - \\\n",
        "            (original_block.number_of_classes * original_block.entropy -\n",
        "             left_block.number_of_classes * left_block.entropy -\n",
        "             right_block.number_of_classes * right_block.entropy)\n",
        "    gain_sup = math.log2(original_block.size - 1) / original_block.size + delta / original_block.size\n",
        "    return gain_sup\n",
        "\n",
        "\n",
        "# Identify the best acceptable value to split block\n",
        "# block: a block of dataset\n",
        "# Return value: a list of (boundary, entropy gain, left block, right block) or\n",
        "#   None when it's unnecessary to split\n",
        "def split(block):\n",
        "    candidates = [x[0] for x in block.data]     # candidates is a list of values can be picked up as boundary\n",
        "    candidates = list(set(candidates))          # get different values in table\n",
        "    candidates.sort()                           # sort ascending\n",
        "    candidates = candidates[1:]                 # discard smallest, because by definition no value is smaller\n",
        "\n",
        "    wall = []       # wall is a list storing final boundary\n",
        "    for value in candidates:\n",
        "        # split by value into 2 groups, below & above\n",
        "        left_data = []\n",
        "        right_data = []\n",
        "        for data_case in block.data:\n",
        "            if data_case[0] < value:\n",
        "                left_data.append(data_case)\n",
        "            else:\n",
        "                right_data.append(data_case)\n",
        "\n",
        "        left_block = Block(left_data)\n",
        "        right_block = Block(right_data)\n",
        "\n",
        "        gain = entropy_gain(block, left_block, right_block)\n",
        "        threshold = min_gain(block, left_block, right_block)\n",
        "\n",
        "        # minimum threshold is met, the value is an acceptable candidate\n",
        "        if gain >= threshold:\n",
        "            wall.append([value, gain, left_block, right_block])\n",
        "\n",
        "    if wall:    # has candidate\n",
        "        wall.sort(key=lambda wall: wall[1], reverse=True)   # sort descending by \"gain\"\n",
        "        return wall[0]      # return best candidate with max entropy gain\n",
        "    else:\n",
        "        return None         # no need to split\n",
        "\n",
        "\n",
        "# Top-down recursive partition of a data block, append boundary into \"walls\"\n",
        "# block: a data block\n",
        "def partition(block):\n",
        "    walls = []\n",
        "\n",
        "    # inner recursive function, accumulate the partitioning values\n",
        "    # sub_block: just a data block\n",
        "    def recursive_split(sub_block):\n",
        "        wall_returned = split(sub_block)        # binary partition, get bin boundary\n",
        "        if wall_returned:                       # still can be spilt\n",
        "            walls.append(wall_returned[0])      # record this partitioning value\n",
        "            recursive_split(wall_returned[2])   # recursively process left block\n",
        "            recursive_split(wall_returned[3])   # recursively split right block\n",
        "        else:\n",
        "            return                              # end of recursion\n",
        "\n",
        "    recursive_split(block)      # call inner function\n",
        "    walls.sort()                # sort boundaries descending\n",
        "    return walls\n",
        "\n"
      ],
      "metadata": {
        "id": "aV62JmL089WA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Description: Pre-process original data. Firstly, we process the missing values (donated as '?'), discarding this column\n",
        "    when missing ratio above 50%, or filling blanks when below. We \"guess\" missing values by simply filling the mode of\n",
        "    existing values in the same column. And then, for the numerical attribute, we discretizate it by recursive minimal\n",
        "    entropy partitioning (see rmep.py). For the categorical attribute, we just replace the label with a\n",
        "    positive integer. For more information, see [1].\n",
        "Input: a data table with several data case, many attributes and class label in the last column, a list of the name of\n",
        "    each attribute, and a list of the type of each column.\n",
        "Output: a data list without numerical values and \"str\" categorical values.\n",
        "Author: CBA Studio\n",
        "Reference:\n",
        "    1. http://cgi.csc.liv.ac.uk/~frans/KDD/Software/LUCS-KDD-DN/lucs-kdd_DN.html\n",
        "\"\"\"\n",
        "# Identify the mode of a list, both effective for numerical and categorical list. When there exists too many modes\n",
        "#   having the same frequency, return the first one.\n",
        "# arr: a list need to find mode\n",
        "def get_mode(arr):\n",
        "    mode = []\n",
        "    arr_appear = dict((a, arr.count(a)) for a in arr)   # count appearance times of each key\n",
        "    if max(arr_appear.values()) == 1:       # if max time is 1\n",
        "        return      # no mode here\n",
        "    else:\n",
        "        for k, v in arr_appear.items():     # else, mode is the number which has max time\n",
        "            if v == max(arr_appear.values()):\n",
        "                mode.append(k)\n",
        "    return mode[0]  # return first number if has many modes\n",
        "\n",
        "\n",
        "# Fill missing values in column column_no, when missing values ration below 50%.\n",
        "# data: original data list\n",
        "# column_no: identify the column No. of that to be filled\n",
        "def fill_missing_values(data, column_no):\n",
        "    size = len(data)\n",
        "    column_data = [x[column_no] for x in data]      # get that column\n",
        "    while '?' in column_data:\n",
        "        column_data.remove('?')\n",
        "    mode = get_mode(column_data)\n",
        "    for i in range(size):\n",
        "        if data[i][column_no] == '?':\n",
        "            data[i][column_no] = mode              # fill in mode\n",
        "    return data\n",
        "\n",
        "\n",
        "# Get the list needed by rmep.py, just glue the data column with class column.\n",
        "# data_column: the data column\n",
        "# class_column: the class label column\n",
        "def get_discretization_data(data_column, class_column):\n",
        "    size = len(data_column)\n",
        "    result_list = []\n",
        "    for i in range(size):\n",
        "        result_list.append([data_column[i], class_column[i]])\n",
        "    return result_list\n",
        "\n",
        "\n",
        "# Replace numerical data with the No. of interval, i.e. consecutive positive integers.\n",
        "# data: original data table\n",
        "# column_no: the column No. of that column\n",
        "# walls: the split point of the whole range\n",
        "def replace_numerical(data, column_no, walls):\n",
        "    size = len(data)\n",
        "    num_spilt_point = len(walls)\n",
        "    for i in range(size):\n",
        "        if data[i][column_no] > walls[num_spilt_point - 1]:\n",
        "            data[i][column_no] = num_spilt_point + 1\n",
        "            continue\n",
        "        for j in range(0, num_spilt_point):\n",
        "            if data[i][column_no] <= walls[j]:\n",
        "                data[i][column_no] = j + 1\n",
        "                break\n",
        "    return data\n",
        "\n",
        "\n",
        "# Replace categorical values with a positive integer.\n",
        "# data: original data table\n",
        "# column_no: identify which column to be processed\n",
        "def replace_categorical(data, column_no):\n",
        "    size = len(data)\n",
        "    classes = set([x[column_no] for x in data])\n",
        "    classes_no = dict([(label, 0) for label in classes])\n",
        "    j = 1\n",
        "    for i in classes:\n",
        "        classes_no[i] = j\n",
        "        j += 1\n",
        "    for i in range(size):\n",
        "        data[i][column_no] = classes_no[data[i][column_no]]\n",
        "    return data, classes_no\n",
        "\n",
        "\n",
        "# Discard all the column with its column_no in discard_list\n",
        "# data: original data set\n",
        "# discard_list: a list of column No. of the columns to be discarded\n",
        "def discard(data, discard_list):\n",
        "    size = len(data)\n",
        "    length = len(data[0])\n",
        "    data_result = []\n",
        "    for i in range(size):\n",
        "        data_result.append([])\n",
        "        for j in range(length):\n",
        "            if j not in discard_list:\n",
        "                data_result[i].append(data[i][j])\n",
        "    return data_result\n",
        "\n",
        "\n",
        "# Main method here, see Description in detail\n",
        "# data: original data table\n",
        "# attribute: a list of the name of attribute\n",
        "# value_type: a list identifying the type of each column\n",
        "# Returned value: a data table after process\n",
        "def pre_process(data, attribute, value_type):\n",
        "    column_num = len(data[0])\n",
        "    size = len(data)\n",
        "    class_column = [x[-1] for x in data]\n",
        "    discard_list = []\n",
        "    for i in range(0, column_num - 1):\n",
        "        data_column = [x[i] for x in data]\n",
        "\n",
        "        # process missing values\n",
        "        missing_values_ratio = data_column.count('?') / size\n",
        "        if missing_values_ratio > 0.5:\n",
        "            discard_list.append(i)\n",
        "            continue\n",
        "        elif missing_values_ratio > 0:\n",
        "            data = fill_missing_values(data, i)\n",
        "            data_column = [x[i] for x in data]\n",
        "\n",
        "        # discretization\n",
        "        if value_type[i] == 'numerical':\n",
        "            discretization_data = get_discretization_data(data_column, class_column)\n",
        "            block = Block(discretization_data)\n",
        "            walls = partition(block)\n",
        "            if len(walls) == 0:\n",
        "                max_value = max(data_column)\n",
        "                min_value = min(data_column)\n",
        "                step = (max_value - min_value) / 3\n",
        "                walls.append(min_value + step)\n",
        "                walls.append(min_value + 2 * step)\n",
        "            print(attribute[i] + \":\", walls)        # print out split points\n",
        "            data = replace_numerical(data, i, walls)\n",
        "        elif value_type[i] == 'categorical':\n",
        "            data, classes_no = replace_categorical(data, i)\n",
        "            print(attribute[i] + \":\", classes_no)   # print out replacement list\n",
        "\n",
        "    # discard\n",
        "    if len(discard_list) > 0:\n",
        "        data = discard(data, discard_list)\n",
        "        print(\"discard:\", discard_list)             # print out discard list\n",
        "    return data\n"
      ],
      "metadata": {
        "id": "-phdEn0m85Ar"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing data"
      ],
      "metadata": {
        "id": "blEFJeTUXHgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iris = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/Data Analytics/iris.data', header = None)\n",
        "bank = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/Data Analytics/banknote_authentication.data', header = None)\n",
        "diabetes = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/Data Analytics/pima-indians-diabetes.data', header = None)\n"
      ],
      "metadata": {
        "id": "Q5xHNoGix5Mh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transactions = iris.values\n",
        "# transactions = bank.values\n",
        "transactions = diabetes.values"
      ],
      "metadata": {
        "id": "rrmvREopyHNM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iris Dataset\n",
        "# test_attribute = ['sepal length','sepal width','petal length','petal width','class']\n",
        "# test_value_type = ['numerical','numerical','numerical','numerical','label']\n",
        "# test_data_after = pre_process(transactions, test_attribute, test_value_type)\n",
        "# test_data_after\n",
        "\n",
        "# Bank Dataset\n",
        "# test_attribute = ['variance of Wavelet Transformed image','skewness of Wavelet Transformed image','curtosis of Wavelet Transformed image','entropy of image','class']\n",
        "# test_value_type = ['numerical','numerical','numerical','numerical','label']\n",
        "# test_data_after = pre_process(transactions, test_attribute, test_value_type)\n",
        "# test_data_after\n",
        "\n",
        "# Diabetes Dataset\n",
        "test_attribute = ['Number of times pregnant','Plasma glucose concentration a 2 hours in an oral glucose tolerance test','Diastolic blood pressure','Triceps skin fold thickness','2-Hour serum insulin','Body mass index','Diabetes pedigree function','Age','class']\n",
        "test_value_type = ['numerical','numerical','numerical','numerical','numerical','numerical','numerical','numerical','label']\n",
        "test_data_after = pre_process(transactions, test_attribute, test_value_type)\n",
        "test_data_after\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSgr8xzx9QFS",
        "outputId": "639db38d-0f2f-4573-e7f4-8241c1a74076"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of times pregnant: [7.0]\n",
            "Plasma glucose concentration a 2 hours in an oral glucose tolerance test: [100.0, 128.0, 155.0]\n",
            "Diastolic blood pressure: [40.666666666666664, 81.33333333333333]\n",
            "Triceps skin fold thickness: [33.0, 66.0]\n",
            "2-Hour serum insulin: [15.0, 122.0]\n",
            "Body mass index: [27.9]\n",
            "Diabetes pedigree function: [0.528]\n",
            "Age: [29.0]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 3., 2., ..., 2., 2., 1.],\n",
              "       [1., 1., 2., ..., 1., 2., 0.],\n",
              "       [2., 4., 2., ..., 2., 2., 1.],\n",
              "       ...,\n",
              "       [1., 2., 2., ..., 1., 2., 0.],\n",
              "       [1., 2., 2., ..., 1., 2., 1.],\n",
              "       [1., 1., 2., ..., 1., 1., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CBG - RG Algorithm"
      ],
      "metadata": {
        "id": "K_18wQWg-0HQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CBA_RG():\n",
        "  # Generate the large frequent item step\n",
        "  def __init__(self, min_sup, transactions, confidence):\n",
        "    self.min_sup = min_sup\n",
        "    self.min_conf = confidence\n",
        "    # Pass in an array of transactions - Generate freq 1-itemset\n",
        "    freq_data_set = {}\n",
        "    self.transaction_length = len(transactions)\n",
        "    self.total_rules_seen = 0\n",
        "    # First Pass\n",
        "    item_sets, rules_up_count = self.first_pass(transactions)\n",
        "    car_1 , scores = self.gen_rules(item_sets,rules_up_count)\n",
        "    myResults = self.get_freq_items_list(item_sets, True)\n",
        "    self.first = myResults\n",
        "    self.all_car,self.get_all_scores = self.next_pass(myResults, transactions)\n",
        "    self.car_first = car_1\n",
        "    self.scores = scores\n",
        "    # <(condset, condsupCount), (y, rulesupCount)>\n",
        "    # Optional Rule Pruning could be done here\n",
        "  \n",
        "  # Rule Itemset -> {column_cat, value, label : count}\n",
        "  def first_pass(self, data):\n",
        "    item_sets = {}\n",
        "    rules_up_count = {}\n",
        "    for transaction in data:\n",
        "      label = transaction[-1]\n",
        "      for col in range(len(transaction)-1):\n",
        "        # <(condset, condsupCount), (y, rulesupCount)>\n",
        "        itemset = (col, transaction[col])\n",
        "        temp = item_sets.get(itemset, 0) + 1 # update the cond count\n",
        "        tempRuleUpCount = rules_up_count.get((itemset,label), 0) + 1 # update the rules up count\n",
        "        item_sets[itemset] = temp\n",
        "        rules_up_count[(itemset,label)] = tempRuleUpCount\n",
        "    for x in rules_up_count.copy():\n",
        "      if (rules_up_count[x]/self.transaction_length) < self.min_sup: # There could be 2 rules that has same cond \n",
        "        del rules_up_count[x]\n",
        "  \n",
        "    # We look for the cond support for item_sets now -> If rules_up_count doesn't have it, we delete freq\n",
        "    for x in item_sets.copy():\n",
        "      delete = True\n",
        "      for y in rules_up_count: # If there exists an existing rule, then we keep this frequent set \n",
        "        if y[0] == x:\n",
        "          delete = False\n",
        "      \n",
        "      if delete:\n",
        "        del item_sets[x]\n",
        "    # item_sets keep track of condset & condsUpCount\n",
        "    # rules_up_count keeps track of rules count\n",
        "    return item_sets, rules_up_count\n",
        "\n",
        "  def gen_freq_rules(self, freq_set, rules_set):\n",
        "    for x in rules_set.copy():\n",
        "      # x is the condset (column_1, value)\n",
        "      if rules_set[x]/self.transaction_length < self.min_sup: # Test for Infrequent and delete\n",
        "        if x in rules_set:\n",
        "          del rules_set[x]\n",
        "\n",
        "    for x in freq_set.copy():\n",
        "      delete = True\n",
        "      for y in rules_set: \n",
        "        if y[0] == x: # We can find a matching ruleset \n",
        "          delete = False\n",
        "      \n",
        "      if delete:\n",
        "        del freq_set[x]\n",
        "\n",
        "    return freq_set, rules_set\n",
        "\n",
        "\n",
        "  def gen_rules(self, freq_set, rules_set):\n",
        "    # We know it's frequent, now we need see if it's confident before generating rule\n",
        "    car = {}\n",
        "    car_confid_supp = {}\n",
        "    for x in rules_set:\n",
        "      if self.total_rules_seen % 25000 == 0 and self.total_rules_seen > 0 :\n",
        "        print(\"Total Rules Seen \" + str(self.total_rules_seen))\n",
        "      self.total_rules_seen += 1\n",
        "      if self.total_rules_seen > 80000:\n",
        "        break\n",
        "      # x is the condset (column_1, value)\n",
        "      if rules_set[x]/self.transaction_length >= self.min_sup: # Test for frequent again\n",
        "        if x[0] not in car: # If A -> B not in car, then we assigned it into car and we check for confidence \n",
        "          confidence_temp = rules_set[x]/freq_set[x[0]]\n",
        "          if confidence_temp > self.min_conf: # We want the rule to be accurate as well\n",
        "            car[x[0]] = x[1] # LHS => RHS\n",
        "            car_confid_supp[x] = [rules_set[x]/freq_set[x[0]] , rules_set[x]/self.transaction_length] \n",
        "        else: # TODO: Compare based on confidence to keep one rule\n",
        "          cur_car_right = car[x[0]] # Get the current best rule \n",
        "          temp = (x[0], cur_car_right)\n",
        "          # cur_car_confd = rules_set[temp]\n",
        "          cur_car_confidence = rules_set[temp] / freq_set[x[0]]\n",
        "          cond_score = rules_set[x]/freq_set[x[0]]\n",
        "          if cond_score > cur_car_confidence: \n",
        "            # We change rule - And we know confidence it's greater than min_confd since original set has to be more than and this new set has more than orignial set\n",
        "              car[x[0]] = x[1]\n",
        "              car_confid_supp[x] = [rules_set[x]/freq_set[x[0]], rules_set[x]/self.transaction_length]     \n",
        "    return car, car_confid_supp\n",
        " \n",
        "  # Counts items and class occurences to determine freq 1-itemset\n",
        "  def get_freq_items_list(self, full_set, first=False):\n",
        "      temp_arr = []\n",
        "      for x in full_set:\n",
        "        temp_arr.append([x])\n",
        "      return temp_arr\n",
        "\n",
        "  def next_pass(self , freq_item, transactions):\n",
        "    k = 1\n",
        "    car_rules = []\n",
        "    car_confid_supp_scores = []\n",
        "    transaction_info = self.rule_subset_prepare(transactions)\n",
        "    while freq_item: \n",
        "      candidate_sets = self.candidate_gen(freq_item, freq_item, k) \n",
        "      # Process candidate to tuple \n",
        "      candidate_sets_tuple = self.gen_cand_tuple(candidate_sets)\n",
        "      rules_up_count = {}\n",
        "      hash_candidates = self.generate_hash_candidates(candidate_sets)\n",
        "      count = 0\n",
        "      for transaction in transactions:\n",
        "        # We generate all the subset found \n",
        "        results_candidate = self.rule_subset1(candidate_sets_tuple, transaction, transaction_info[count], k)\n",
        "        # We do an increase in count in both support & rule\n",
        "        for candidate in results_candidate: \n",
        "          temp = hash_candidates[candidate] + 1\n",
        "          hash_candidates[candidate] = temp # Increase in support count\n",
        "          temp1 = rules_up_count.get((candidate, results_candidate[candidate]), 0) + 1\n",
        "          rules_up_count[(candidate, results_candidate[candidate])] = temp1\n",
        "        count += 1\n",
        "      hash_candidates = self.postprocess_hash(hash_candidates)\n",
        "      freq_set, rules_up_count = self.gen_freq_rules(hash_candidates, rules_up_count)\n",
        "      rules, scores = self.gen_rules(freq_set, rules_up_count)\n",
        "      car_rules.append(rules)\n",
        "      car_confid_supp_scores.append(scores)\n",
        "      k += 1\n",
        "      freq_item = [v for v in freq_set.keys()]# Assign dictionary as array\n",
        "      # Candidate Pruning\n",
        "      if self.total_rules_seen > 80000:\n",
        "        print(\"Maximum Rules Reached\")\n",
        "        break\n",
        "    return car_rules, car_confid_supp_scores\n",
        "\n",
        "  def postprocess_hash(self, hash_cand):\n",
        "    for x in hash_cand.copy():\n",
        "      if hash_cand[x] == 0: # Means no support at all\n",
        "        del hash_cand[x]\n",
        "    return hash_cand\n",
        "\n",
        "  def generate_hash_candidates(self, candidate_sets):\n",
        "    myDict = {}\n",
        "    for x in candidate_sets:\n",
        "      myDict[tuple(x)] = 0\n",
        "    return myDict\n",
        "\n",
        "  def rule_subset_prepare(self, transactions):\n",
        "    transaction_info = []\n",
        "    for transaction in transactions: \n",
        "      temp = []\n",
        "      for x in range(len(transaction)-1):\n",
        "        temp.append((x, transaction[x]))\n",
        "      transaction_info.append(temp)\n",
        "    return transaction_info\n",
        "\n",
        "  def rule_subset1(self, candidate_sets, transaction, transaction_row, k):\n",
        "    result = {}\n",
        "    for x in candidate_sets:\n",
        "      satisfy = True\n",
        "      for single_rule in x:\n",
        "        if single_rule[1] != transaction[single_rule[0]]:\n",
        "          # not a support\n",
        "          satisfy = False\n",
        "      if satisfy: \n",
        "        result[x] = transaction[-1]\n",
        "    return result\n",
        "\n",
        "# Helper function to convert candidate set\n",
        "  def gen_cand_tuple(self, candidate_sets):\n",
        "    temp = [] \n",
        "    for x in candidate_sets:\n",
        "      temp.append(tuple(x))\n",
        "    return temp\n",
        "\n",
        "# Generate the candidate sets\n",
        "  def candidate_gen(self, freq_item, freq_item_2, k):\n",
        "    candidate_set = []    # condition of joining is that is should have k-1 elements in common \n",
        "    for x in range(len(freq_item)):\n",
        "      temp = freq_item[x]\n",
        "      for y in range(x+1, len(freq_item_2)):\n",
        "        if x ==y: #Ignore the same key\n",
        "          continue \n",
        "        temp2 = freq_item_2[y]\n",
        "        # Check if the rules are the same\n",
        "        if k == 1:\n",
        "          temp_merged = temp[:]\n",
        "          temp_merged.append(temp2[-1])\n",
        "          temp_merged = tuple(temp_merged)\n",
        "          if temp_merged not in candidate_set:\n",
        "            candidate_set.append(temp_merged)\n",
        "        elif temp[:-1] == temp2[:-1]: # Merge the 2\n",
        "          temp_merged = temp + (temp2[-1],)\n",
        "        # If exists in candidate set then good, else delete\n",
        "          if temp_merged not in candidate_set:\n",
        "            candidate_set.append(temp_merged)\n",
        "    return candidate_set\n",
        "\n",
        "  def post_process(self): # prepare data for classification \n",
        "    car_first = self.car_first\n",
        "    car_all = self.all_car[:]\n",
        "    scores_all = self.get_all_scores[:]\n",
        "    scores_all.insert(0, self.scores)\n",
        "    count = 0\n",
        "    car_all.insert(0, car_first)\n",
        "    myDict = {}\n",
        "    for x in car_all:\n",
        "      for item in x:\n",
        "        myDict[(item, x[item])] = scores_all[count][(item , x[item])]\n",
        "      count+=1 \n",
        "    newDict = {} # Intermediate dict to format to pandas for easier sorting \n",
        "    count = 0\n",
        "    for x in myDict:\n",
        "      temp = myDict[x] \n",
        "      temp.insert(0,x)\n",
        "      newDict[count] = temp\n",
        "      count += 1\n",
        "\n",
        "    # Sort my rules \n",
        "    new = pd.DataFrame.from_dict(newDict,  orient ='index', columns = ['rule', 'confidence', 'support'])\n",
        "    df = new.sort_values(['confidence','support'], ascending=[False, False])\n",
        "    return df.values \n"
      ],
      "metadata": {
        "id": "57DNzfcj0Poj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CBA-CB: M1\n"
      ],
      "metadata": {
        "id": "yC60vx_8sJHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CBA_CB_M1:\n",
        "  def __init__(self, my_rules, transactions):\n",
        "    self.my_rules = my_rules\n",
        "    self.transactions = transactions\n",
        "\n",
        "  def generate_classifier(self):\n",
        "    prev_error = []\n",
        "    total_error = []\n",
        "    ruleList = []\n",
        "    default_classes = []\n",
        "    transactions_used = self.transactions[:]\n",
        "    for rule in self.my_rules:\n",
        "      temp = []\n",
        "      actual_rule = rule[0] # Leave out the scores \n",
        "      if type(actual_rule[0][0]) is tuple: # contains only one tuple \n",
        "        single = False \n",
        "      else:\n",
        "        single = True\n",
        "      pred = actual_rule[-1] # gets the label of rule \n",
        "      actual_rule = actual_rule[0] # gets the LHS of rule \n",
        "      count = error = 0\n",
        "      pred_right_once = False\n",
        "      if len(transactions_used) == 0:\n",
        "        break\n",
        "\n",
        "      for transaction in transactions_used:\n",
        "        if single:\n",
        "          if transaction[actual_rule[0]] == actual_rule[1]:\n",
        "            if transaction[-1] == pred: # classify at least one correctly \n",
        "              pred_right_once = True\n",
        "            else:\n",
        "              error += 1\n",
        "            temp.append(count)\n",
        "            # save the marked temp to decide whether to delete at the end or not \n",
        "        else:\n",
        "          for single_col_rule in actual_rule:\n",
        "            satisfy = True\n",
        "            if transaction[single_col_rule[0]] != single_col_rule[1]:\n",
        "              satisfy = False\n",
        "          if satisfy: # Add rule \n",
        "            if transaction[-1] == pred: \n",
        "              pred_right_once = True\n",
        "            else:\n",
        "              error += 1\n",
        "            temp.append(count) # Prepare to delete from list \n",
        "        count += 1\n",
        "\n",
        "      if pred_right_once:\n",
        "      # delete index in transactions\n",
        "        new_transaction = []\n",
        "        temp_counter = 0\n",
        "        for item in transactions_used:\n",
        "          if temp_counter not in temp:\n",
        "              new_transaction.append(item)\n",
        "          temp_counter += 1\n",
        "        transactions_used = new_transaction[:]\n",
        "\n",
        "        # add r to rule_list \n",
        "        ruleList.append((actual_rule, pred))\n",
        "\n",
        "        my_dict = {}\n",
        "        # add majority in remaining as default class\n",
        "        for remaining in new_transaction:\n",
        "          temp = my_dict.get(remaining[-1],0) + 1\n",
        "          my_dict[remaining[-1]] = temp\n",
        "\n",
        "        # find majority class \n",
        "        maj_class = [key for key,value in my_dict.items() if value == max(my_dict.values())]\n",
        "        if len(maj_class) > 0:\n",
        "          maj_class = maj_class[0]\n",
        "        default_classes.append(maj_class)\n",
        "\n",
        "        # calculate total number of error for default class\n",
        "        default_errors = 0\n",
        "        for remaining in new_transaction:\n",
        "          if remaining[-1] == maj_class:\n",
        "            continue\n",
        "          else:\n",
        "            default_errors += 1\n",
        "        # print(\"My error \" + str(error))\n",
        "        # print(\"Default error \" + str(default_errors))\n",
        "        # error is the classes that this rule classify wrongly\n",
        "        if len(prev_error) == 0: \n",
        "            total_error.append(default_errors + error)\n",
        "            prev_error.append(error)\n",
        "        else:\n",
        "            total_error.append(default_errors + error + prev_error[-1])\n",
        "            prev_error.append(sum(prev_error) + error)\n",
        "\n",
        "    return ruleList, prev_error, total_error , default_classes"
      ],
      "metadata": {
        "id": "CKhSRBx1vqsh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Test Set\n",
        "def cba_m1_improved_evaluation(rules, test_set, default_class):\n",
        "  my_pred = []\n",
        "  true_label = []\n",
        "  for test in test_set:\n",
        "    true_label.append(test[-1])\n",
        "    for rule in rules:\n",
        "      actual_rule = rule[0] # Leave out the scores \n",
        "      if type(actual_rule[0]) is tuple: # contains only one tuple \n",
        "        single = False \n",
        "      else:\n",
        "        actual_rule = tuple((actual_rule,))\n",
        "        single = True\n",
        "\n",
        "      pred = rule[-1]\n",
        "      satisfy = True \n",
        "      for single_rule in actual_rule:\n",
        "        if single_rule[1] != test[single_rule[0]]:\n",
        "          satisfy = False\n",
        "      if satisfy:\n",
        "        my_pred.append(pred)# We found the rule and we want to break \n",
        "        break\n",
        "      \n",
        "    if not satisfy: # Means a rule was not found for this test, we resort to default \n",
        "      my_pred.append(default_class) # We predict default_class\n",
        "\n",
        "  return my_pred, true_label\n"
      ],
      "metadata": {
        "id": "PMr0MVcH8iB5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4) Improvement to CBA"
      ],
      "metadata": {
        "id": "d4QUyLNxysgA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "# Helper Function for Ensemble Voting Amongst Weaker Class\n",
        "def majority(arr):\n",
        "  \n",
        "    # convert array into dictionary\n",
        "    freqDict = Counter(arr)\n",
        "  \n",
        "    # traverse dictionary and check majority element\n",
        "    size = len(arr)\n",
        "    for (key,val) in freqDict.items():\n",
        "         if (val > (size/2)):\n",
        "             return key\n"
      ],
      "metadata": {
        "id": "fFdC0l-4_9FQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "# Constant Test_Set to evaluate weak models through 10CV\n",
        "my_final_test = random.sample(test_data_after.tolist(), len(test_data_after)//10)"
      ],
      "metadata": {
        "id": "PoSODaWPRZmq"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 Weak Models "
      ],
      "metadata": {
        "id": "dqz6Iyb7rmeE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "import random\n",
        "\n",
        "avg_error = 0 \n",
        "\n",
        "for _ in range(5):\n",
        "  num_weak_models = 3  # We decide on the number of \"weak model\"\n",
        "  weak_model_pred = []\n",
        "  weak_model_pred_true = []\n",
        "  # We apply bagging by selecting datasets with replacement randomly for each of the weak models\n",
        "  for weak_model in range(num_weak_models):\n",
        "    dataset_len = len(test_data_after)\n",
        "    bagging_data = [] \n",
        "    for x in range(dataset_len):    # We randomly generate the bagging_data/randomly subsample the dataset \n",
        "      idx = random.randint(0, dataset_len-1)\n",
        "      bagging_data.append(test_data_after[idx])\n",
        "\n",
        "    X = [item[:-1] for item in bagging_data]\n",
        "    y = [item[-1] for item in bagging_data]\n",
        "\n",
        "    mean_error_train = 0\n",
        "    mean_rules = 0\n",
        "    mean_rules_classifier = 0\n",
        "    mean_classifier_error = 0\n",
        "    \n",
        "    kf = KFold(n_splits=10, random_state=42, shuffle=True)\n",
        "    kf.get_n_splits(X)\n",
        "    fold_no = 0\n",
        "    weak_model_10_fold_pred = []\n",
        "    weak_model_10_fold_true_pred = []\n",
        "\n",
        "    for train_index, test_index in kf.split(X):\n",
        "      # print(\"Currently in fold \" + str(fold_no))\n",
        "      my_train_data = []\n",
        "      for i in train_index: \n",
        "        temp_list = np.append(X[i], y[i]) \n",
        "        my_train_data.append(temp_list.tolist())\n",
        "\n",
        "      test_list = []\n",
        "      for i in test_index:\n",
        "        temp_list = np.append(X[i], y[i])\n",
        "        test_list.append(temp_list.tolist())\n",
        "\n",
        "      cba_rules = CBA_RG(0.01, my_train_data[:], 0.5) # Rule Generation happens in initialization\n",
        "      # my_rules - Ruleset, Confidence, Min_Sup\n",
        "      my_rules = cba_rules.post_process()\n",
        "      cba_classifier = CBA_CB_M1(my_rules, my_train_data)\n",
        "      ruleList, prev_error , total_error , default_classes= cba_classifier.generate_classifier()\n",
        "      min_errors_idx = total_error.index(min(total_error))\n",
        "      # We take rules up to that min_error rule \n",
        "      mean_classifier_error += min(total_error)\n",
        "      ruleList = ruleList[:min_errors_idx+1]\n",
        "      weak_pred, true_label = cba_m1_improved_evaluation(ruleList, my_final_test, default_classes[min_errors_idx])\n",
        "      weak_model_10_fold_pred = weak_model_10_fold_pred + weak_pred\n",
        "      weak_model_10_fold_true_pred = weak_model_10_fold_true_pred + true_label\n",
        "      fold_no += 1\n",
        "    print(\"10 CV finished\")\n",
        "    weak_model_pred.append(weak_model_10_fold_pred)\n",
        "    weak_model_pred_true.append(weak_model_10_fold_true_pred)\n",
        "\n",
        "  # We do majority voting ensembling for weaker model \n",
        "  error = 0 \n",
        "  for x in range(len(weak_model_pred[0])): # For the test results \n",
        "    ensemble = []\n",
        "    for z in range(len(weak_model_pred)): # For three model\n",
        "      ensemble.append(weak_model_pred[z][x])\n",
        "    # Find majority element and compare with true label \n",
        "    maj_class = majority(ensemble)\n",
        "    if maj_class != weak_model_pred_true[0][x]: # Compare Majority Pred with the True Label\n",
        "      error += 1 \n",
        "    \n",
        "  print(error/10) # This is the error for ensemble majority voting for one experiment\n",
        "  avg_error += (error/10) \n",
        "print(avg_error/5) # Repeat Experiment 5 times to reduce variance due to randomness of weaker model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1zSh5PFpo3c",
        "outputId": "f733ccee-9c9c-4faa-d8aa-35a63ed3e27c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 CV finished\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "25.7\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "38.0\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "31.8\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "37.8\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "36.0\n",
            "33.86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 Weak Models"
      ],
      "metadata": {
        "id": "K-6AkN5ir1yC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "import random\n",
        "avg_error = 0 \n",
        "\n",
        "for _ in range(5):\n",
        "  num_weak_models = 5 # We decide on the number of \"weak model\"\n",
        "  weak_model_pred = []\n",
        "  weak_model_pred_true = []\n",
        "  # We apply bagging by selecting datasets with replacement randomly for each of the weak models\n",
        "  for weak_model in range(num_weak_models):\n",
        "    # Randomly subsample the dataset \n",
        "    dataset_len = len(test_data_after)\n",
        "    bagging_data = [] \n",
        "    # Keep bagging data constant \n",
        "    for x in range(dataset_len):    # We randomly generate the bagging_data\n",
        "      idx = random.randint(0, dataset_len-1)\n",
        "      bagging_data.append(test_data_after[idx])\n",
        "\n",
        "    X = [item[:-1] for item in bagging_data]\n",
        "    y = [item[-1] for item in bagging_data]\n",
        "\n",
        "    mean_error_train = 0\n",
        "    mean_rules = 0\n",
        "    mean_rules_classifier = 0\n",
        "    mean_classifier_error = 0\n",
        "    \n",
        "    kf = KFold(n_splits=10, random_state=42, shuffle=True)\n",
        "    kf.get_n_splits(X)\n",
        "    fold_no = 0\n",
        "    weak_model_10_fold_pred = []\n",
        "    weak_model_10_fold_true_pred = []\n",
        "\n",
        "    for train_index, test_index in kf.split(X):\n",
        "      # print(\"Currently in fold \" + str(fold_no))\n",
        "      my_train_data = []\n",
        "      for i in train_index: \n",
        "        temp_list = np.append(X[i], y[i]) \n",
        "        my_train_data.append(temp_list.tolist())\n",
        "\n",
        "      test_list = []\n",
        "      for i in test_index:\n",
        "        temp_list = np.append(X[i], y[i])\n",
        "        test_list.append(temp_list.tolist())\n",
        "\n",
        "      cba_rules = CBA_RG(0.01, my_train_data[:], 0.5) # Rule Generation happens in initialization\n",
        "      \n",
        "      # my_rules - Ruleset, Confidence, Min_Sup\n",
        "      my_rules = cba_rules.post_process() \n",
        "      cba_classifier = CBA_CB_M1(my_rules, my_train_data)\n",
        "      ruleList, prev_error , total_error , default_classes= cba_classifier.generate_classifier()\n",
        "      min_errors_idx = total_error.index(min(total_error))\n",
        "      # We take rules up to that min_error rule \n",
        "      mean_classifier_error += min(total_error)\n",
        "      ruleList = ruleList[:min_errors_idx+1]\n",
        "      weak_pred, true_label = cba_m1_improved_evaluation(ruleList, my_final_test, default_classes[min_errors_idx])\n",
        "      weak_model_10_fold_pred = weak_model_10_fold_pred + weak_pred\n",
        "      weak_model_10_fold_true_pred = weak_model_10_fold_true_pred + true_label\n",
        "      fold_no += 1\n",
        "    print(\"10 CV finished\")\n",
        "    # 10 fold CV done - We add to weak_model_pred\n",
        "    weak_model_pred.append(weak_model_10_fold_pred)\n",
        "    weak_model_pred_true.append(weak_model_10_fold_true_pred)\n",
        "\n",
        "  # We do majority voting ensembling for weaker model \n",
        "  error = 0 \n",
        "  for x in range(len(weak_model_pred[0])): # For the test results \n",
        "    ensemble = []\n",
        "    for z in range(len(weak_model_pred)): # For five model\n",
        "      ensemble.append(weak_model_pred[z][x])\n",
        "    # Find majority element and compare with true label\n",
        "    maj_class = majority(ensemble)\n",
        "    if maj_class != weak_model_pred_true[0][x]: # Compare Majority voting for one experiment\n",
        "      error += 1 \n",
        "    \n",
        "  print(error/10)\n",
        "  avg_error += (error/10) \n",
        "print(avg_error/5) # Repeat Experiment 5 times to reduce variance due to randomness of weaker model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwuA2z5dKWCC",
        "outputId": "b96aedee-69da-4bb1-c48d-c42cd5453df1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 CV finished\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "34.2\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "39.7\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "34.6\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "32.6\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "37.8\n",
            "35.779999999999994\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7 Weak Models"
      ],
      "metadata": {
        "id": "pjRD79ERr3gq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split Data into 10 sets\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "import random\n",
        "avg_error = 0 \n",
        "\n",
        "for _ in range(5):\n",
        "  num_weak_models = 7 # We decide on the number of \"weak model\"\n",
        "  weak_model_pred = []\n",
        "  weak_model_pred_true = []\n",
        "  # We apply bagging by selecting datasets with replacement randomly for each of the weak models\n",
        "  for weak_model in range(num_weak_models): # We randomly generate the bagging_data/randomly subsample the dataset \n",
        "    dataset_len = len(test_data_after)\n",
        "    bagging_data = [] \n",
        "    for x in range(dataset_len):    # We randomly generate the bagging_data\n",
        "      idx = random.randint(0, dataset_len-1)\n",
        "      bagging_data.append(test_data_after[idx])\n",
        "\n",
        "    X = [item[:-1] for item in bagging_data]\n",
        "    y = [item[-1] for item in bagging_data]\n",
        "\n",
        "    mean_error_train = 0\n",
        "    mean_rules = 0\n",
        "    mean_rules_classifier = 0\n",
        "    mean_classifier_error = 0\n",
        "    \n",
        "    kf = KFold(n_splits=10, random_state=42, shuffle=True)\n",
        "    kf.get_n_splits(X)\n",
        "    fold_no = 0\n",
        "    weak_model_10_fold_pred = []\n",
        "    weak_model_10_fold_true_pred = []\n",
        "\n",
        "    for train_index, test_index in kf.split(X):\n",
        "      # print(\"Currently in fold \" + str(fold_no))\n",
        "      my_train_data = []\n",
        "      for i in train_index: \n",
        "        temp_list = np.append(X[i], y[i]) \n",
        "        my_train_data.append(temp_list.tolist())\n",
        "\n",
        "      test_list = []\n",
        "      for i in test_index:\n",
        "        temp_list = np.append(X[i], y[i])\n",
        "        test_list.append(temp_list.tolist())\n",
        "\n",
        "      cba_rules = CBA_RG(0.01, my_train_data[:], 0.5) # Rule Generation happens in initialization\n",
        "      # my_rules - Ruleset, Confidence, Min_Sup\n",
        "      my_rules = cba_rules.post_process() \n",
        "      cba_classifier = CBA_CB_M1(my_rules, my_train_data)\n",
        "      ruleList, prev_error , total_error , default_classes= cba_classifier.generate_classifier()\n",
        "      min_errors_idx = total_error.index(min(total_error))\n",
        "      # We take rules up to that min_error rule \n",
        "      mean_classifier_error += min(total_error)\n",
        "      ruleList = ruleList[:min_errors_idx+1]\n",
        "      weak_pred, true_label = cba_m1_improved_evaluation(ruleList, my_final_test, default_classes[min_errors_idx])\n",
        "      weak_model_10_fold_pred = weak_model_10_fold_pred + weak_pred\n",
        "      weak_model_10_fold_true_pred = weak_model_10_fold_true_pred + true_label\n",
        "      fold_no += 1\n",
        "    print(\"10 CV finished\")\n",
        "    # 10 fold CV done - We add to weak_model_pred\n",
        "    weak_model_pred.append(weak_model_10_fold_pred)\n",
        "    weak_model_pred_true.append(weak_model_10_fold_true_pred)\n",
        "  # We do majority voting ensembling for weaker model and calculate error \n",
        "  error = 0 \n",
        "  for x in range(len(weak_model_pred[0])): \n",
        "    ensemble = []\n",
        "    for z in range(len(weak_model_pred)): # For 7 model\n",
        "      ensemble.append(weak_model_pred[z][x])\n",
        "    # Find majority element and compare with weak_model_pred_true[0][x]\n",
        "    maj_class = majority(ensemble)\n",
        "    if maj_class != weak_model_pred_true[0][x]:\n",
        "      error += 1 \n",
        "    \n",
        "  print(error/10)\n",
        "  avg_error += (error/10) \n",
        "print(avg_error/5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUi5ZgE7N2ct",
        "outputId": "631cd0d1-6599-4141-ed76-edd725ad5c03"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 CV finished\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "36.6\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "34.9\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "35.5\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "30.8\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "10 CV finished\n",
            "29.8\n",
            "33.52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Original 1 Weak Model"
      ],
      "metadata": {
        "id": "C04_W5pur5XA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split Data into 10 sets\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "import random\n",
        "avg_error = 0 \n",
        "\n",
        "# We apply bagging by selecting datasets with replacement randomly ~ \n",
        "# We then train this dataset with 10CV \n",
        "for _ in range(5):\n",
        "  # We decide on the number of \"weak model\"\n",
        "  num_weak_models = 1\n",
        "  weak_model_pred = []\n",
        "  weak_model_pred_true = []\n",
        "  # We shall sample 3 diff datasets each time \n",
        "  for weak_model in range(num_weak_models):\n",
        "    bagging_data = test_data_after[:]\n",
        "    X = [item[:-1] for item in bagging_data]\n",
        "    y = [item[-1] for item in bagging_data]\n",
        "\n",
        "    mean_error_train = 0\n",
        "    mean_rules = 0\n",
        "    mean_rules_classifier = 0\n",
        "    mean_classifier_error = 0\n",
        "    \n",
        "    kf = KFold(n_splits=10, random_state=42, shuffle=True)\n",
        "    kf.get_n_splits(X)\n",
        "    fold_no = 0\n",
        "    weak_model_10_fold_pred = []\n",
        "    weak_model_10_fold_true_pred = []\n",
        "\n",
        "    for train_index, test_index in kf.split(X):\n",
        "      # print(\"Currently in fold \" + str(fold_no))\n",
        "      my_train_data = []\n",
        "      for i in train_index: \n",
        "        temp_list = np.append(X[i], y[i]) \n",
        "        my_train_data.append(temp_list.tolist())\n",
        "\n",
        "      test_list = []\n",
        "      for i in test_index:\n",
        "        temp_list = np.append(X[i], y[i])\n",
        "        test_list.append(temp_list.tolist())\n",
        "\n",
        "      cba_rules = CBA_RG(0.01, my_train_data[:], 0.5) # Rule Generation happens in initialization\n",
        "      \n",
        "      # my_rules - Ruleset, Confidence, Min_Sup\n",
        "      my_rules = cba_rules.post_process() \n",
        "      cba_classifier = CBA_CB_M1(my_rules, my_train_data)\n",
        "      ruleList, prev_error , total_error , default_classes= cba_classifier.generate_classifier()\n",
        "      min_errors_idx = total_error.index(min(total_error))\n",
        "      # We take rules up to that min_error rule \n",
        "      mean_classifier_error += min(total_error)\n",
        "      ruleList = ruleList[:min_errors_idx+1]\n",
        "      weak_pred, true_label = cba_m1_improved_evaluation(ruleList, my_final_test, default_classes[min_errors_idx])\n",
        "      # print(\"Printing my prediciotn\")\n",
        "      weak_model_10_fold_pred = weak_model_10_fold_pred + weak_pred\n",
        "      weak_model_10_fold_true_pred = weak_model_10_fold_true_pred + true_label\n",
        "      fold_no += 1\n",
        "    print(\"10 CV finished\")\n",
        "    weak_model_pred.append(weak_model_10_fold_pred)\n",
        "    weak_model_pred_true.append(weak_model_10_fold_true_pred)\n",
        "\n",
        "  # After all the iteration, we then do ensemble majority voting\n",
        "  error = 0 \n",
        "  for x in range(len(weak_model_pred[0])): # For the test results \n",
        "    ensemble = []\n",
        "    for z in range(len(weak_model_pred)): # For 1 model\n",
        "      ensemble.append(weak_model_pred[z][x])\n",
        "    # Find majority element and compare with weak_model_pred_true[0][x]\n",
        "    maj_class = majority(ensemble)\n",
        "    if maj_class != weak_model_pred_true[0][x]:\n",
        "      error += 1 \n",
        "    \n",
        "  print(error/10)\n",
        "  avg_error += (error/10) \n",
        "  # If 1 Model -> 38 errors\n",
        "print(avg_error/5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVHPCAneKswb",
        "outputId": "4bcb1a57-0290-400a-934a-d11ab835cc17"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 CV finished\n",
            "41.4\n",
            "10 CV finished\n",
            "41.4\n",
            "10 CV finished\n",
            "41.4\n",
            "10 CV finished\n",
            "41.4\n",
            "10 CV finished\n",
            "41.4\n",
            "41.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wp6FWycQ7r4H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}